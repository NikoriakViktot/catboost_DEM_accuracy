{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-01T10:53:24.622670Z",
     "start_time": "2025-10-01T10:53:22.772318Z"
    }
   },
   "source": [
    "import duckdb, pandas as pd, numpy as np\n",
    "\n",
    "file_train_cls = 'data/NMAD_with_embeddings_cls.parquet'\n",
    "con = duckdb.connect(file_train_cls)\n",
    "\n",
    "# забираємо все потрібне під FABDEM\n",
    "df = con.execute(\"\"\"\n",
    "  SELECT\n",
    "    -- групування / просторові поля\n",
    "    rgt, track, spot, x, y,\n",
    "    -- таргет FABDEM\n",
    "    cls_nmad_fab, nmad_fab,\n",
    "    h_fab_dem,\n",
    "    fab_dem_slope, fab_dem_twi, fab_dem_2000, fab_dem_stream,\n",
    "\n",
    "    -- категоріальні LULC/geomorphon/landform\n",
    "    lulc_class, lulc_name, fab_dem_geomorphon, fab_dem_landform,\n",
    "    -- ембедінги (64 колонок або emb_all)\n",
    "    emb_001, emb_002, emb_003, emb_004, emb_005, emb_006, emb_007, emb_008,\n",
    "    emb_009, emb_010, emb_011, emb_012, emb_013, emb_014, emb_015, emb_016,\n",
    "    emb_017, emb_018, emb_019, emb_020, emb_021, emb_022, emb_023, emb_024,\n",
    "    emb_025, emb_026, emb_027, emb_028, emb_029, emb_030, emb_031, emb_032,\n",
    "    emb_033, emb_034, emb_035, emb_036, emb_037, emb_038, emb_039, emb_040,\n",
    "    emb_041, emb_042, emb_043, emb_044, emb_045, emb_046, emb_047, emb_048,\n",
    "    emb_049, emb_050, emb_051, emb_052, emb_053, emb_054, emb_055, emb_056,\n",
    "    emb_057, emb_058, emb_059, emb_060, emb_061, emb_062, emb_063, emb_064,\n",
    "    -- або emb_all якщо зберігав як масив/список\n",
    "  FROM NMAD_with_embeddings_cls\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "# Таргет\n",
    "df[\"cls_nmad_fab\"] = df[\"cls_nmad_fab\"].astype(int)\n",
    "\n",
    "# Група для spatial split\n",
    "df[\"group_id\"] = (df[\"rgt\"].astype(int).astype(str) + \"_\" +\n",
    "                  df[\"track\"].astype(int).astype(str))\n"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T10:53:27.986714Z",
     "start_time": "2025-10-01T10:53:27.487623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# рахуємо розмір груп\n",
    "gsize = df.groupby(\"group_id\").size().rename(\"grp_n\")  # <-- унікальна назва!\n",
    "\n",
    "# приєднуємо розмір груп до df (без конфлікту імен)\n",
    "df = df.join(gsize, on=\"group_id\")\n",
    "\n",
    "MIN_TEST_N = 3000\n",
    "df[\"is_big_group\"] = df[\"grp_n\"] >= MIN_TEST_N\n"
   ],
   "id": "2d88761d4066968a",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T10:53:31.730952Z",
     "start_time": "2025-10-01T10:53:31.728360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_cols = [\n",
    "    \"x\", \"y\",\"h_fab_dem\",\n",
    "    \"fab_dem_slope\",\"fab_dem_twi\",\"fab_dem_2000\",\"fab_dem_stream\"\n",
    "]\n",
    "cat_cols = [\"lulc_class\",\"lulc_name\",\"fab_dem_geomorphon\",\"fab_dem_landform\"]\n",
    "\n",
    "# ембедінги (якщо є всі 64 канали окремими колонками)\n",
    "emb_cols = [f\"emb_{i:03d}\" for i in range(1,65) if f\"emb_{i:03d}\" in df.columns]\n"
   ],
   "id": "bff3d8dca8bce0cc",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T10:53:37.014701Z",
     "start_time": "2025-10-01T10:53:35.796654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def collapse_rare(df, col, min_frac=0.01):\n",
    "    vc = df[col].value_counts(normalize=True)\n",
    "    rare = vc[vc < min_frac].index\n",
    "    df[col] = df[col].where(~df[col].isin(rare), \"__OTHER__\")\n",
    "    return df\n",
    "\n",
    "for c in cat_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(str)\n",
    "        df = collapse_rare(df, c, 0.01)\n"
   ],
   "id": "614bebf6af7f6d07",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T10:53:52.711919Z",
     "start_time": "2025-10-01T10:53:46.281132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "y = df[\"cls_nmad_fab\"].astype(int)\n",
    "groups = df[\"group_id\"]\n",
    "is_big = df[\"is_big_group\"].values\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "folds = []\n",
    "big_idx = np.where(is_big)[0]\n",
    "y_big   = y.iloc[big_idx]\n",
    "grp_big = groups.iloc[big_idx]\n",
    "\n",
    "for fold, (tr_big, te_big) in enumerate(sgkf.split(big_idx, y_big, groups=grp_big), 1):\n",
    "    tr_idx = big_idx[tr_big]\n",
    "    te_idx = big_idx[te_big]\n",
    "    # усі \"малі\" групи — тільки в train\n",
    "    tr_idx = np.concatenate([tr_idx, np.where(~is_big)[0]])\n",
    "    folds.append((np.sort(tr_idx), np.sort(te_idx)))\n",
    "\n",
    "    print(\n",
    "        f\"Fold {fold}: \"\n",
    "        f\"train groups = {df.iloc[tr_idx]['group_id'].nunique()} | \"\n",
    "        f\"test groups = {df.iloc[te_idx]['group_id'].nunique()}\"\n",
    "    )\n",
    "\n",
    "# Перевірки\n",
    "for k, (tr, te) in enumerate(folds, 1):\n",
    "    assert set(df.iloc[tr][\"group_id\"]).isdisjoint(set(df.iloc[te][\"group_id\"]))\n",
    "    print(f\"Fold {k} class dist train:\",\n",
    "          y.iloc[tr].value_counts(normalize=True).sort_index().round(3).to_dict(),\n",
    "          \"| test:\",\n",
    "          y.iloc[te].value_counts(normalize=True).sort_index().round(3).to_dict())\n"
   ],
   "id": "bbd5dca3d0d3d51c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: train groups = 8 | test groups = 3\n",
      "Fold 2: train groups = 8 | test groups = 3\n",
      "Fold 3: train groups = 9 | test groups = 2\n",
      "Fold 4: train groups = 10 | test groups = 1\n",
      "Fold 5: train groups = 10 | test groups = 1\n",
      "Fold 1 class dist train: {0: 0.332, 1: 0.329, 2: 0.34} | test: {0: 0.326, 1: 0.332, 2: 0.342}\n",
      "Fold 2 class dist train: {0: 0.326, 1: 0.332, 2: 0.343} | test: {0: 0.338, 1: 0.326, 2: 0.337}\n",
      "Fold 3 class dist train: {0: 0.331, 1: 0.333, 2: 0.336} | test: {0: 0.319, 1: 0.308, 2: 0.372}\n",
      "Fold 4 class dist train: {0: 0.33, 1: 0.326, 2: 0.344} | test: {0: 0.328, 1: 0.359, 2: 0.313}\n",
      "Fold 5 class dist train: {0: 0.33, 1: 0.33, 2: 0.34} | test: {0: 0.33, 1: 0.327, 2: 0.343}\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T11:06:59.190211Z",
     "start_time": "2025-10-01T11:06:58.887169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ембедінги (64-колонки)\n",
    "Z = df[emb_cols].to_numpy(dtype=\"float32\")\n",
    "\n",
    "# PCA з 32 компонентами\n",
    "pca = PCA(n_components=24, random_state=42)\n",
    "pca.fit(Z)\n",
    "\n",
    "# explained variance ratio (по кожному компоненту)\n",
    "evr = pca.explained_variance_ratio_\n",
    "\n",
    "# кумулятивна сума — скільки % дисперсії накопичилось\n",
    "print(\"EVR (по кожній компоненті):\", evr)\n",
    "print(\"EVR cumulative (24):\", evr.cumsum()[-1])\n"
   ],
   "id": "2fd162e1b1f6de61",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVR (по кожній компоненті): [0.34533104 0.13338497 0.10327472 0.09242915 0.0495403  0.0450718\n",
      " 0.03297483 0.02686134 0.02141951 0.01786984 0.01509036 0.01256596\n",
      " 0.01103213 0.00947172 0.00858145 0.00828346 0.00690836 0.00612718\n",
      " 0.00570824 0.00465606 0.00392586 0.00368238 0.0031934  0.00287302]\n",
      "EVR cumulative (24): 0.970257\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T11:10:21.174303Z",
     "start_time": "2025-10-01T11:08:08.214634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score, confusion_matrix, classification_report\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "\n",
    "# --- налаштування ---\n",
    "SAVE_DIR = \"artifacts_fabdem_cv\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "REDUCED_DIM = 48  # скільки компонент залишати з 64-ембедінгів\n",
    "\n",
    "\n",
    "# --- підготовка PCA об'єкта (фітитимемо на train усередині 1-го фолду) ---\n",
    "pca = PCA(n_components=REDUCED_DIM, random_state=42) if len(emb_cols) > 0 else None\n",
    "\n",
    "\n",
    "all_fold_metrics = []\n",
    "best_model = None\n",
    "best_macro_f1 = -1.0\n",
    "best_fold_id = None\n",
    "\n",
    "# для збереження усіх тестових прогнозів по фолдах\n",
    "test_preds_concat = []\n",
    "\n",
    "for k, (tr, te) in enumerate(folds, 1):\n",
    "    print(f\"\\n===== FOLD {k} =====\")\n",
    "    y_tr, y_te = y.iloc[tr], y.iloc[te]\n",
    "\n",
    "    # числові/категоріальні\n",
    "    Xn_tr = df.loc[tr, num_cols].copy()\n",
    "    Xn_te = df.loc[te, num_cols].copy()\n",
    "    Xc_tr = df.loc[tr, cat_cols].astype(str).copy()\n",
    "    Xc_te = df.loc[te, cat_cols].astype(str).copy()\n",
    "\n",
    "    # ембедінги + PCA (fit на train у першому фолді, далі тільки transform)\n",
    "    if len(emb_cols) > 0:\n",
    "        Z_tr = df.loc[tr, emb_cols].to_numpy(dtype=\"float32\")\n",
    "        Z_te = df.loc[te, emb_cols].to_numpy(dtype=\"float32\")\n",
    "        if k == 1:\n",
    "            pca.fit(Z_tr)\n",
    "        Z_tr = pca.transform(Z_tr)\n",
    "        Z_te = pca.transform(Z_te)\n",
    "        Ztr_df = pd.DataFrame(Z_tr, index=Xn_tr.index, columns=[f\"pca_emb_{i:02d}\" for i in range(REDUCED_DIM)])\n",
    "        Zte_df = pd.DataFrame(Z_te, index=Xn_te.index, columns=[f\"pca_emb_{i:02d}\" for i in range(REDUCED_DIM)])\n",
    "        Xn_tr = pd.concat([Xn_tr, Ztr_df], axis=1)\n",
    "        Xn_te = pd.concat([Xn_te, Zte_df], axis=1)\n",
    "\n",
    "    # фінальна матриця фіч\n",
    "    X_tr = pd.concat([Xn_tr, Xc_tr], axis=1)\n",
    "    X_te = pd.concat([Xn_te, Xc_te], axis=1)\n",
    "    cat_idx = [X_tr.columns.get_loc(c) for c in Xc_tr.columns]\n",
    "\n",
    "    # пули\n",
    "    train_pool = Pool(X_tr, y_tr, cat_features=cat_idx)\n",
    "    test_pool  = Pool(X_te, y_te, cat_features=cat_idx)\n",
    "\n",
    "    # модель\n",
    "    model = CatBoostClassifier(\n",
    "        loss_function=\"MultiClass\",\n",
    "        iterations=3000, learning_rate=0.05, depth=8,\n",
    "        l2_leaf_reg=8, auto_class_weights=\"Balanced\",\n",
    "        random_state=42, task_type=\"GPU\",\n",
    "        od_type=\"Iter\", od_wait=100, verbose=200\n",
    "    )\n",
    "    model.fit(train_pool, eval_set=test_pool, use_best_model=True)\n",
    "\n",
    "    # прогнози\n",
    "    y_pred = model.predict(test_pool).astype(int).ravel()\n",
    "    y_proba = model.predict_proba(test_pool)  # shape: (N, 3)\n",
    "    macro_f1 = f1_score(y_te, y_pred, average=\"macro\")\n",
    "    bal_acc  = balanced_accuracy_score(y_te, y_pred)\n",
    "\n",
    "    # per-class F1\n",
    "    f1_per_class = f1_score(y_te, y_pred, average=None)  # порядок класів 0,1,2\n",
    "    cm = confusion_matrix(y_te, y_pred, labels=[0,1,2])\n",
    "\n",
    "    print(f\"FOLD {k}  macro-F1={macro_f1:.3f}  balanced-acc={bal_acc:.3f}\")\n",
    "    print(\"Per-class F1:\", {c: round(v,3) for c, v in zip([0,1,2], f1_per_class)})\n",
    "    print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
    "\n",
    "    # зберегти метрики фолду\n",
    "    all_fold_metrics.append({\n",
    "        \"fold\": k,\n",
    "        \"macro_f1\": float(macro_f1),\n",
    "        \"balanced_acc\": float(bal_acc),\n",
    "        \"f1_class_0\": float(f1_per_class[0]),\n",
    "        \"f1_class_1\": float(f1_per_class[1]),\n",
    "        \"f1_class_2\": float(f1_per_class[2]),\n",
    "        \"cm_00\": int(cm[0,0]), \"cm_01\": int(cm[0,1]), \"cm_02\": int(cm[0,2]),\n",
    "        \"cm_10\": int(cm[1,0]), \"cm_11\": int(cm[1,1]), \"cm_12\": int(cm[1,2]),\n",
    "        \"cm_20\": int(cm[2,0]), \"cm_21\": int(cm[2,1]), \"cm_22\": int(cm[2,2]),\n",
    "    })\n",
    "\n",
    "    # трекінг найкращої моделі\n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        best_model = model\n",
    "        best_fold_id = k\n",
    "\n",
    "    # збережемо тестові прогнози цього фолду з координатами — для карти та аналізу\n",
    "    out_te = df.loc[te, [\"x\",\"y\",\"rgt\",\"track\",\"spot\"]].copy()\n",
    "    out_te[\"y_true\"] = y_te.values\n",
    "    out_te[\"y_pred\"] = y_pred\n",
    "    out_te[\"prob_low\"]  = y_proba[:,0]\n",
    "    out_te[\"prob_mid\"]  = y_proba[:,1]\n",
    "    out_te[\"prob_high\"] = y_proba[:,2]\n",
    "    out_te[\"fold\"] = k\n",
    "    test_preds_concat.append(out_te)\n",
    "\n",
    "# --- підсумки по фолдах ---\n",
    "metrics_df = pd.DataFrame(all_fold_metrics)\n",
    "metrics_df.to_csv(os.path.join(SAVE_DIR, \"cv_metrics.csv\"), index=False)\n",
    "\n",
    "print(\"\\n===== CV SUMMARY =====\")\n",
    "for col in [\"macro_f1\", \"balanced_acc\", \"f1_class_0\", \"f1_class_1\", \"f1_class_2\"]:\n",
    "    mu, sd = metrics_df[col].mean(), metrics_df[col].std()\n",
    "    print(f\"{col}: {mu:.3f} ± {sd:.3f}\")\n",
    "\n",
    "print(f\"\\nBest fold = {best_fold_id}  (macro-F1={best_macro_f1:.3f})\")\n",
    "\n",
    "# збереження найкращої моделі та (за потреби) PCA + маніфест колонок\n",
    "best_model.save_model(os.path.join(SAVE_DIR, \"catboost_fabdem_best.cbm\"))\n",
    "if pca is not None:\n",
    "    joblib.dump(pca, os.path.join(SAVE_DIR, \"pca_embeddings.pkl\"))\n",
    "\n",
    "columns_manifest = {\n",
    "    \"numeric\": list(X_tr.select_dtypes(include=[np.number]).columns),\n",
    "    \"categorical\": list(Xc_tr.columns),\n",
    "    \"target\": \"cls_nmad_fab\",\n",
    "    \"pca_dim\": REDUCED_DIM if pca is not None else 0\n",
    "}\n",
    "json.dump(columns_manifest, open(os.path.join(SAVE_DIR, \"columns_manifest.json\"), \"w\"), ensure_ascii=False, indent=2)\n",
    "\n",
    "# усі тестові прогнози разом (для швидкої перевірки на карті)\n",
    "test_preds_df = pd.concat(test_preds_concat, axis=0).reset_index(drop=True)\n",
    "test_preds_df.to_parquet(os.path.join(SAVE_DIR, \"cv_test_predictions.parquet\"))\n",
    "print(\"\\nSaved:\")\n",
    "print(\" - CV metrics ->\", os.path.join(SAVE_DIR, \"cv_metrics.csv\"))\n",
    "print(\" - Best model ->\", os.path.join(SAVE_DIR, \"catboost_fabdem_best.cbm\"))\n",
    "print(\" - Columns manifest ->\", os.path.join(SAVE_DIR, \"columns_manifest.json\"))\n",
    "print(\" - PCA (optional) ->\", os.path.join(SAVE_DIR, \"pca_embeddings.pkl\"))\n",
    "print(\" - Test preds ->\", os.path.join(SAVE_DIR, \"cv_test_predictions.parquet\"))\n"
   ],
   "id": "f16623977ac094a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FOLD 1 =====\n",
      "0:\tlearn: 1.0874247\ttest: 1.0902800\tbest: 1.0902800 (0)\ttotal: 44.5ms\tremaining: 2m 13s\n",
      "200:\tlearn: 0.8453768\ttest: 0.9756905\tbest: 0.9755776 (197)\ttotal: 7.46s\tremaining: 1m 43s\n",
      "400:\tlearn: 0.7888125\ttest: 0.9741398\tbest: 0.9736605 (329)\ttotal: 14.9s\tremaining: 1m 36s\n",
      "bestTest = 0.97366049\n",
      "bestIteration = 329\n",
      "Shrink model to first 330 iterations.\n",
      "FOLD 1  macro-F1=0.498  balanced-acc=0.505\n",
      "Per-class F1: {0: np.float64(0.458), 1: np.float64(0.415), 2: np.float64(0.622)}\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[109723  51772  62554]\n",
      " [ 93994  79947  54476]\n",
      " [ 51113  25192 158858]]\n",
      "\n",
      "===== FOLD 2 =====\n",
      "0:\tlearn: 1.0870629\ttest: 1.0889423\tbest: 1.0889423 (0)\ttotal: 38.4ms\tremaining: 1m 55s\n",
      "200:\tlearn: 0.8348082\ttest: 0.9689741\tbest: 0.9689741 (200)\ttotal: 7.44s\tremaining: 1m 43s\n",
      "bestTest = 0.9683644348\n",
      "bestIteration = 279\n",
      "Shrink model to first 280 iterations.\n",
      "FOLD 2  macro-F1=0.503  balanced-acc=0.514\n",
      "Per-class F1: {0: np.float64(0.461), 1: np.float64(0.4), 2: np.float64(0.648)}\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[115529  56202  68332]\n",
      " [ 98413  76607  56739]\n",
      " [ 46687  18063 174626]]\n",
      "\n",
      "===== FOLD 3 =====\n",
      "0:\tlearn: 1.0876146\ttest: 1.0873459\tbest: 1.0873459 (0)\ttotal: 79.1ms\tremaining: 3m 57s\n",
      "200:\tlearn: 0.8580773\ttest: 0.9529770\tbest: 0.9529770 (200)\ttotal: 9.08s\tremaining: 2m 6s\n",
      "400:\tlearn: 0.8022874\ttest: 0.9479385\tbest: 0.9479385 (400)\ttotal: 18s\tremaining: 1m 56s\n",
      "600:\tlearn: 0.7642344\ttest: 0.9451501\tbest: 0.9449728 (598)\ttotal: 26.9s\tremaining: 1m 47s\n",
      "800:\tlearn: 0.7354478\ttest: 0.9446720\tbest: 0.9443424 (734)\ttotal: 35.8s\tremaining: 1m 38s\n",
      "bestTest = 0.9443423639\n",
      "bestIteration = 734\n",
      "Shrink model to first 735 iterations.\n",
      "FOLD 3  macro-F1=0.516  balanced-acc=0.522\n",
      "Per-class F1: {0: np.float64(0.432), 1: np.float64(0.44), 2: np.float64(0.677)}\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[32162 20731 22941]\n",
      " [26421 28721 18088]\n",
      " [14394  7793 66220]]\n",
      "\n",
      "===== FOLD 4 =====\n",
      "0:\tlearn: 1.0880995\ttest: 1.0879534\tbest: 1.0879534 (0)\ttotal: 77ms\tremaining: 3m 50s\n",
      "200:\tlearn: 0.8601735\ttest: 0.9587401\tbest: 0.9586886 (192)\ttotal: 9.34s\tremaining: 2m 10s\n",
      "400:\tlearn: 0.8056753\ttest: 0.9574616\tbest: 0.9560910 (324)\ttotal: 18.4s\tremaining: 1m 59s\n",
      "bestTest = 0.9560910107\n",
      "bestIteration = 324\n",
      "Shrink model to first 325 iterations.\n",
      "FOLD 4  macro-F1=0.536  balanced-acc=0.539\n",
      "Per-class F1: {0: np.float64(0.477), 1: np.float64(0.519), 2: np.float64(0.614)}\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[35031 23279 16727]\n",
      " [24768 41649 15784]\n",
      " [12052 13502 46117]]\n",
      "\n",
      "===== FOLD 5 =====\n",
      "0:\tlearn: 1.0871940\ttest: 1.0904176\tbest: 1.0904176 (0)\ttotal: 78.9ms\tremaining: 3m 56s\n",
      "200:\tlearn: 0.8583261\ttest: 0.9800327\tbest: 0.9800327 (200)\ttotal: 9.51s\tremaining: 2m 12s\n",
      "bestTest = 0.9800327355\n",
      "bestIteration = 200\n",
      "Shrink model to first 201 iterations.\n",
      "FOLD 5  macro-F1=0.501  balanced-acc=0.510\n",
      "Per-class F1: {0: np.float64(0.419), 1: np.float64(0.452), 2: np.float64(0.633)}\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[20695 13350 17598]\n",
      " [17217 20344 13501]\n",
      " [ 9167  5231 39170]]\n",
      "\n",
      "===== CV SUMMARY =====\n",
      "macro_f1: 0.511 ± 0.016\n",
      "balanced_acc: 0.518 ± 0.013\n",
      "f1_class_0: 0.450 ± 0.023\n",
      "f1_class_1: 0.445 ± 0.046\n",
      "f1_class_2: 0.639 ± 0.025\n",
      "\n",
      "Best fold = 4  (macro-F1=0.536)\n",
      "\n",
      "Saved:\n",
      " - CV metrics -> artifacts_fabdem_cv\\cv_metrics.csv\n",
      " - Best model -> artifacts_fabdem_cv\\catboost_fabdem_best.cbm\n",
      " - Columns manifest -> artifacts_fabdem_cv\\columns_manifest.json\n",
      " - PCA (optional) -> artifacts_fabdem_cv\\pca_embeddings.pkl\n",
      " - Test preds -> artifacts_fabdem_cv\\cv_test_predictions.parquet\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T11:10:44.033646Z",
     "start_time": "2025-10-01T11:10:35.769418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# 1) читаємо parquet\n",
    "df = pd.read_parquet(\"artifacts_fabdem_cv/cv_test_predictions.parquet\")\n",
    "\n",
    "# 2) прапор помилки (плутає клас)\n",
    "df[\"err\"] = (df[\"y_true\"] != df[\"y_pred\"]).astype(int)\n",
    "\n",
    "# 3) GeoDataFrame у метрах (EPSG:3857)\n",
    "gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df[\"x\"], df[\"y\"]), crs=\"EPSG:4326\")\n",
    "\n",
    "# 4) для folium треба широту/довготу → трансформуємо в WGS84\n",
    "gdf_wgs = gdf.to_crs(4326)\n",
    "\n",
    "# 5) беремо тільки помилки (або зважуємо вагою err)\n",
    "pts = gdf_wgs.loc[gdf_wgs[\"err\"] == 1, [\"geometry\"]].copy()\n",
    "pts[\"lat\"] = pts.geometry.y\n",
    "pts[\"lon\"] = pts.geometry.x\n",
    "\n",
    "# 6) стартова карта\n",
    "center = [pts[\"lat\"].median(), pts[\"lon\"].median()] if len(pts) else [48.5, 31.2]\n",
    "m = folium.Map(location=center, zoom_start=6, tiles=\"CartoDB positron\")\n",
    "\n",
    "# 7) теплокарта помилок (кожна помилка вагою 1)\n",
    "heat_data = pts[[\"lat\",\"lon\"]].values.tolist()\n",
    "HeatMap(heat_data, radius=10, blur=15, min_opacity=0.3).add_to(m)\n",
    "\n",
    "# 8) опц.: додати точки з найбільшою P(high) серед усього тесту\n",
    "top = gdf_wgs.sort_values(\"prob_high\", ascending=False).head(2000)\n",
    "for _, r in top.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[r.geometry.y, r.geometry.x],\n",
    "        radius=3, opacity=0.6, fill=True, fill_opacity=0.6,\n",
    "        popup=f\"y_true={r.y_true}, y_pred={r.y_pred}, Phigh={r.prob_high:.2f}\"\n",
    "    ).add_to(m)\n",
    "\n",
    "m.save(\"error_heatmap.html\")\n",
    "print(\"Saved error_heatmap.html\")\n"
   ],
   "id": "312d35e56f8379c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved error_heatmap.html\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T11:10:44.136511Z",
     "start_time": "2025-10-01T11:10:44.086817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"artifacts_fabdem_cv/cv_test_predictions.parquet\")\n",
    "print(df.columns.tolist())\n",
    "print(df.head())"
   ],
   "id": "88ae8c78c48f068c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x', 'y', 'rgt', 'track', 'spot', 'y_true', 'y_pred', 'prob_low', 'prob_mid', 'prob_high', 'fold']\n",
      "           x          y    rgt  track  spot  y_true  y_pred  prob_low  \\\n",
      "0  25.061067  47.736959  396.0    3.0   5.0       0       0  0.454448   \n",
      "1  25.061067  47.736959  396.0    3.0   5.0       0       0  0.454448   \n",
      "2  25.061066  47.736966  396.0    3.0   5.0       0       0  0.454448   \n",
      "3  25.061066  47.736966  396.0    3.0   5.0       0       0  0.454448   \n",
      "4  25.061054  47.737061  396.0    3.0   5.0       0       0  0.427440   \n",
      "\n",
      "   prob_mid  prob_high  fold  \n",
      "0  0.353362   0.192189     1  \n",
      "1  0.353362   0.192189     1  \n",
      "2  0.353362   0.192189     1  \n",
      "3  0.353362   0.192189     1  \n",
      "4  0.327311   0.245249     1  \n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T09:55:31.082895Z",
     "start_time": "2025-10-01T09:55:30.411292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_parquet(\"artifacts_fabdem_cv/cv_test_predictions.parquet\")\n",
    "\n",
    "df[\"model_error\"] = (df[\"y_true\"] != df[\"y_pred\"]).astype(int)\n",
    "df[\"uncertainty\"] = 1.0 - df[[\"prob_low\",\"prob_mid\",\"prob_high\"]].max(axis=1)\n",
    "\n",
    "df.to_parquet(\"artifacts_fabdem_cv/cv_test_predictions_with_flags.parquet\", index=False)\n"
   ],
   "id": "690e65447b286729",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T09:55:45.958370Z",
     "start_time": "2025-10-01T09:55:45.694319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "df = pd.read_parquet(\"artifacts_fabdem_cv/cv_test_predictions_with_flags.parquet\")\n",
    "\n",
    "# конвертація 3857 -> 4326\n",
    "\n",
    "\n",
    "df[\"lon\"] = lon\n",
    "df[\"lat\"] = lat\n",
    "\n",
    "# тільки помилки\n",
    "err = df[df[\"model_error\"] == 1][[\"lat\",\"lon\"]].dropna()\n",
    "\n",
    "m = folium.Map(location=[df[\"lat\"].median(), df[\"lon\"].median()],\n",
    "               zoom_start=10, tiles=\"CartoDB positron\")\n",
    "\n",
    "HeatMap(err.values.tolist(), radius=12, blur=18, max_zoom=12).add_to(m)\n",
    "m.save(\"fabdem_errors_heatmap.html\")\n",
    "print(\"Saved -> fabdem_errors_heatmap.html\")\n",
    "\n"
   ],
   "id": "4b51348d14ce2ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved -> fabdem_errors_heatmap.html\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T09:56:00.522435Z",
     "start_time": "2025-10-01T09:56:00.135227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "unc = df.sort_values(\"uncertainty\", ascending=False).head(20000)  # топ-20k\n",
    "m2 = folium.Map(location=[df[\"lat\"].median(), df[\"lon\"].median()],\n",
    "                zoom_start=10, tiles=\"CartoDB positron\")\n",
    "HeatMap(unc[[\"lat\",\"lon\",\"uncertainty\"]].values.tolist(), radius=13, blur=20).add_to(m2)\n",
    "m2.save(\"fabdem_uncertainty_heatmap.html\")\n"
   ],
   "id": "d20a0cd68cb0e965",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T09:56:14.957650Z",
     "start_time": "2025-10-01T09:56:11.454485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hi = df[(df[\"y_pred\"] == 2) & (df[\"prob_high\"] >= 0.8)]\n",
    "m3 = folium.Map(location=[df[\"lat\"].median(), df[\"lon\"].median()],\n",
    "                zoom_start=10, tiles=\"CartoDB positron\")\n",
    "HeatMap(hi[[\"lat\",\"lon\",\"prob_high\"]].values.tolist(), radius=10, blur=16).add_to(m3)\n",
    "m3.save(\"fabdem_high_class_hotspots.html\")"
   ],
   "id": "b65823e3772ac7f4",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T09:59:28.649093Z",
     "start_time": "2025-10-01T09:59:28.646582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# приклади категоріальних оглядів\n",
    "for col in [\"lulc_name\", \"fab_dem_landform\", \"fab_dem_geomorphon\"]:\n",
    "    if col in df.columns:\n",
    "        top = (df.groupby(col)[\"model_error\"]\n",
    "                 .mean()\n",
    "                 .sort_values(ascending=False)\n",
    "                 .head(15))\n",
    "        print(f\"\\nTop error-rate by {col}:\\n\", top)\n",
    "\n",
    "# по рельєфу/нахилах (числові)\n",
    "if \"fab_dem_slope\" in df.columns:\n",
    "    bins = pd.cut(df[\"fab_dem_slope\"], bins=[0,5,10,15,25,35,90])\n",
    "    print(\"\\nError-rate by slope bins:\\n\",\n",
    "          df.groupby(bins)[\"model_error\"].mean())\n"
   ],
   "id": "72ee3038b31f7c40",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T10:13:18.930562Z",
     "start_time": "2025-10-01T10:13:18.732030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_parquet(\"artifacts_fabdem_cv/cv_test_predictions.parquet\")\n",
    "\n",
    "# 1) Бінарна помилка\n",
    "df[\"is_error\"] = (df[\"y_true\"] != df[\"y_pred\"]).astype(int)\n",
    "\n",
    "# 2) Невизначеність (чим ближче до 1 → тим модель менш впевнена)\n",
    "df[\"uncertainty\"] = 1 - df[[\"prob_low\", \"prob_mid\", \"prob_high\"]].max(axis=1)\n",
    "\n",
    "# 3) High-class hotspot (приклад: якщо y_true == 2, і модель часто плутає)\n",
    "df[\"is_high_class\"] = ((df[\"y_true\"] == 2) & (df[\"is_error\"] == 1)).astype(int)\n"
   ],
   "id": "9add2c8438220c96",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T11:15:09.500864Z",
     "start_time": "2025-10-01T11:14:19.560744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from pyproj import Transformer\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# 1) Дані CV\n",
    "df = pd.read_parquet(\"artifacts_fabdem_cv/cv_test_predictions.parquet\")\n",
    "gdf3857 = gpd.GeoDataFrame(\n",
    "    df.copy(),\n",
    "    geometry=gpd.points_from_xy(df[\"x\"], df[\"y\"]),\n",
    "    crs=\"EPSG:4326\"     # ВАЖЛИВО: явно вказати\n",
    ")\n",
    "\n",
    "# 2) Семплінг HAND (растр у 32635). Координати перетворюємо лише для семплінгу\n",
    "def sample_raster_to_points(gdf_src, raster_path, out_col):\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        rast_crs = src.crs\n",
    "        nodata = src.nodata\n",
    "        tr = Transformer.from_crs(gdf_src.crs, rast_crs, always_xy=True)\n",
    "\n",
    "        xs = gdf_src.geometry.x.values\n",
    "        ys = gdf_src.geometry.y.values\n",
    "        xr, yr = tr.transform(xs, ys)     # => координати в CRS растру\n",
    "\n",
    "        vals = np.array(list(src.sample(zip(xr, yr))))  # (N, bands)\n",
    "        if vals.ndim == 2 and vals.shape[1] == 1:\n",
    "            vals = vals[:, 0]\n",
    "        if nodata is not None:\n",
    "            vals = np.where(np.isclose(vals, nodata), np.nan, vals)\n",
    "\n",
    "    gdf_out = gdf_src.copy()\n",
    "    gdf_out[out_col] = vals\n",
    "    return gdf_out\n",
    "\n",
    "gdf3857 = sample_raster_to_points(\n",
    "    gdf3857, \"data/fab_dem_utm32635_hand_2000.tif\", out_col=\"hand\"\n",
    ")\n",
    "\n",
    "# 3) Підготовка ваг/фільтрів\n",
    "gdf3857[\"is_error\"] = (gdf3857[\"y_true\"] != gdf3857[\"y_pred\"]).astype(int)\n",
    "gdf3857[\"uncertainty\"] = 1 - gdf3857[[\"prob_low\",\"prob_mid\",\"prob_high\"]].max(axis=1)\n",
    "\n",
    "# 4) ПЕРЕД МАПОЮ: проектуємо в WGS84!\n",
    "gdf4326 = gdf3857.to_crs(4326)\n",
    "\n",
    "def save_heatmap(gdf, out_html, value_col=None, radius=14, blur=22):\n",
    "    m = folium.Map(\n",
    "        location=[gdf.geometry.y.mean(), gdf.geometry.x.mean()],\n",
    "        zoom_start=10, tiles=\"cartodbpositron\"\n",
    "    )\n",
    "    if value_col is None:\n",
    "        data = list(zip(gdf.geometry.y, gdf.geometry.x))\n",
    "    else:\n",
    "        data = list(zip(gdf.geometry.y, gdf.geometry.x, gdf[value_col].fillna(0.0)))\n",
    "    HeatMap(data, radius=radius, blur=blur, max_zoom=13).add_to(m)\n",
    "    m.save(out_html)\n",
    "\n",
    "# 5) Приклади карт\n",
    "#   а) невизначеність моделі\n",
    "save_heatmap(gdf4326, \"fabdem_uncertainty_heatmap.html\", value_col=\"uncertainty\")\n",
    "\n",
    "#   б) тільки помилки\n",
    "save_heatmap(gdf4326[gdf4326[\"is_error\"] == 1], \"fabdem_errors_heatmap.html\")\n",
    "\n",
    "#   в) помилки в зонах HAND < 5 м (як приклад)\n",
    "save_heatmap(gdf4326[(gdf4326[\"is_error\"] == 1) & (gdf4326[\"hand\"] < 5)],\n",
    "             \"fabdem_errors_floodplain.html\")\n"
   ],
   "id": "e86981a7a9f9521f",
   "outputs": [],
   "execution_count": 42
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
