{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T11:52:04.551346Z",
     "start_time": "2025-10-01T11:52:04.475048Z"
    }
   },
   "cell_type": "code",
   "source": "import duckdb",
   "id": "d0950ac153566b2b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-01T11:54:25.181370Z",
     "start_time": "2025-10-01T11:54:24.941919Z"
    }
   },
   "source": [
    "file_train_cls = 'data/NMAD_with_embeddings_cls_features.parquet'\n",
    "con = duckdb.connect(file_train_cls)\n",
    "df = con.execute(\"select * from NMAD_with_embeddings_cls_features limit 10\").df()\n",
    "print(df.head(5))\n",
    "print(df.describe())\n",
    "print(df.columns)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     rgt  track  spot       height  geoid_height  orthometric_height  \\\n",
      "0  556.0    1.0   5.0  1069.885864        36.636         1033.249864   \n",
      "1  556.0    1.0   5.0  1069.885864        36.636         1033.249864   \n",
      "2  556.0    1.0   5.0  1069.440063        36.636         1032.804063   \n",
      "3  556.0    1.0   5.0  1069.440063        36.636         1032.804063   \n",
      "4  556.0    1.0   5.0  1070.128174        36.636         1033.492174   \n",
      "\n",
      "   h_alos_dem  delta_alos_dem  abs_delta_alos_dem  h_aster_dem  ...  \\\n",
      "0      1054.0       20.750136           20.750136       1046.0  ...   \n",
      "1      1054.0       20.750136           20.750136       1046.0  ...   \n",
      "2      1044.0       11.195937           11.195937       1046.0  ...   \n",
      "3      1044.0       11.195937           11.195937       1046.0  ...   \n",
      "4      1044.0       10.507826           10.507826       1046.0  ...   \n",
      "\n",
      "   cls_nmad_fab  curvature  roughness       tpi       tri   d8_accum  \\\n",
      "0             0   0.490112  14.673096  1.189575  16.19342  53.221977   \n",
      "1             0   0.490112  14.673096  1.189575  16.19342  53.221977   \n",
      "2             1   0.490112  14.673096  1.189575  16.19342  53.221977   \n",
      "3             1   0.490112  14.673096  1.189575  16.19342  53.221977   \n",
      "4             0   0.490112  14.673096  1.189575  16.19342  53.221977   \n",
      "\n",
      "   fab_breached  aspect_sin  aspect_cos  d8_accum_log1p  \n",
      "0   1034.973267   -0.001413   -0.999999        3.993086  \n",
      "1   1034.973267   -0.001413   -0.999999        3.993086  \n",
      "2   1034.973267   -0.001413   -0.999999        3.993086  \n",
      "3   1034.973267   -0.001413   -0.999999        3.993086  \n",
      "4   1034.973267   -0.001413   -0.999999        3.993086  \n",
      "\n",
      "[5 rows x 157 columns]\n",
      "         rgt  track  spot       height  geoid_height  orthometric_height  \\\n",
      "count   10.0   10.0  10.0    10.000000  1.000000e+01           10.000000   \n",
      "mean   556.0    1.0   5.0  1068.447559  3.663600e+01         1031.811559   \n",
      "std      0.0    0.0   0.0     1.787303  7.489778e-15            1.787303   \n",
      "min    556.0    1.0   5.0  1066.242065  3.663600e+01         1029.606065   \n",
      "25%    556.0    1.0   5.0  1066.541626  3.663600e+01         1029.905626   \n",
      "50%    556.0    1.0   5.0  1069.440063  3.663600e+01         1032.804063   \n",
      "75%    556.0    1.0   5.0  1069.885864  3.663600e+01         1033.249864   \n",
      "max    556.0    1.0   5.0  1070.128174  3.663600e+01         1033.492174   \n",
      "\n",
      "       h_alos_dem  delta_alos_dem  abs_delta_alos_dem  h_aster_dem  ...  \\\n",
      "count    10.00000       10.000000           10.000000         10.0  ...   \n",
      "mean   1046.00000       14.188441           14.188441       1046.0  ...   \n",
      "std       4.21637        3.818345            3.818345          0.0  ...   \n",
      "min    1044.00000       10.507826           10.507826       1046.0  ...   \n",
      "25%    1044.00000       11.195937           11.195937       1046.0  ...   \n",
      "50%    1044.00000       14.094374           14.094374       1046.0  ...   \n",
      "75%    1044.00000       14.393935           14.393935       1046.0  ...   \n",
      "max    1054.00000       20.750136           20.750136       1046.0  ...   \n",
      "\n",
      "       cls_nmad_fab  curvature  roughness        tpi       tri   d8_accum  \\\n",
      "count     10.000000  10.000000  10.000000  10.000000  10.00000  10.000000   \n",
      "mean       1.000000   0.490112  14.673096   1.189575  16.19342  53.221977   \n",
      "std        0.942809   0.000000   0.000000   0.000000   0.00000   0.000000   \n",
      "min        0.000000   0.490112  14.673096   1.189575  16.19342  53.221977   \n",
      "25%        0.000000   0.490112  14.673096   1.189575  16.19342  53.221977   \n",
      "50%        1.000000   0.490112  14.673096   1.189575  16.19342  53.221977   \n",
      "75%        2.000000   0.490112  14.673096   1.189575  16.19342  53.221977   \n",
      "max        2.000000   0.490112  14.673096   1.189575  16.19342  53.221977   \n",
      "\n",
      "       fab_breached  aspect_sin  aspect_cos  d8_accum_log1p  \n",
      "count     10.000000   10.000000   10.000000       10.000000  \n",
      "mean    1034.973389   -0.001413   -0.999999        3.993087  \n",
      "std        0.000000    0.000000    0.000000        0.000000  \n",
      "min     1034.973267   -0.001413   -0.999999        3.993086  \n",
      "25%     1034.973267   -0.001413   -0.999999        3.993086  \n",
      "50%     1034.973267   -0.001413   -0.999999        3.993086  \n",
      "75%     1034.973267   -0.001413   -0.999999        3.993086  \n",
      "max     1034.973267   -0.001413   -0.999999        3.993086  \n",
      "\n",
      "[8 rows x 148 columns]\n",
      "Index(['rgt', 'track', 'spot', 'height', 'geoid_height', 'orthometric_height',\n",
      "       'h_alos_dem', 'delta_alos_dem', 'abs_delta_alos_dem', 'h_aster_dem',\n",
      "       ...\n",
      "       'cls_nmad_fab', 'curvature', 'roughness', 'tpi', 'tri', 'd8_accum',\n",
      "       'fab_breached', 'aspect_sin', 'aspect_cos', 'd8_accum_log1p'],\n",
      "      dtype='object', length=157)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T11:58:50.832689Z",
     "start_time": "2025-10-01T11:58:50.823152Z"
    }
   },
   "cell_type": "code",
   "source": "df",
   "id": "f4b50c865996f03e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     rgt  track  spot       height  geoid_height  orthometric_height  \\\n",
       "0  556.0    1.0   5.0  1069.885864        36.636         1033.249864   \n",
       "1  556.0    1.0   5.0  1069.885864        36.636         1033.249864   \n",
       "2  556.0    1.0   5.0  1069.440063        36.636         1032.804063   \n",
       "3  556.0    1.0   5.0  1069.440063        36.636         1032.804063   \n",
       "4  556.0    1.0   5.0  1070.128174        36.636         1033.492174   \n",
       "5  556.0    1.0   5.0  1070.128174        36.636         1033.492174   \n",
       "6  556.0    1.0   5.0  1066.541626        36.636         1029.905626   \n",
       "7  556.0    1.0   5.0  1066.242065        36.636         1029.606065   \n",
       "8  556.0    1.0   5.0  1066.541626        36.636         1029.905626   \n",
       "9  556.0    1.0   5.0  1066.242065        36.636         1029.606065   \n",
       "\n",
       "   h_alos_dem  delta_alos_dem  abs_delta_alos_dem  h_aster_dem  ...  \\\n",
       "0      1054.0       20.750136           20.750136       1046.0  ...   \n",
       "1      1054.0       20.750136           20.750136       1046.0  ...   \n",
       "2      1044.0       11.195937           11.195937       1046.0  ...   \n",
       "3      1044.0       11.195937           11.195937       1046.0  ...   \n",
       "4      1044.0       10.507826           10.507826       1046.0  ...   \n",
       "5      1044.0       10.507826           10.507826       1046.0  ...   \n",
       "6      1044.0       14.094374           14.094374       1046.0  ...   \n",
       "7      1044.0       14.393935           14.393935       1046.0  ...   \n",
       "8      1044.0       14.094374           14.094374       1046.0  ...   \n",
       "9      1044.0       14.393935           14.393935       1046.0  ...   \n",
       "\n",
       "   cls_nmad_fab  curvature  roughness       tpi       tri   d8_accum  \\\n",
       "0             0   0.490112  14.673096  1.189575  16.19342  53.221977   \n",
       "1             0   0.490112  14.673096  1.189575  16.19342  53.221977   \n",
       "2             1   0.490112  14.673096  1.189575  16.19342  53.221977   \n",
       "3             1   0.490112  14.673096  1.189575  16.19342  53.221977   \n",
       "4             0   0.490112  14.673096  1.189575  16.19342  53.221977   \n",
       "5             0   0.490112  14.673096  1.189575  16.19342  53.221977   \n",
       "6             2   0.490112  14.673096  1.189575  16.19342  53.221977   \n",
       "7             2   0.490112  14.673096  1.189575  16.19342  53.221977   \n",
       "8             2   0.490112  14.673096  1.189575  16.19342  53.221977   \n",
       "9             2   0.490112  14.673096  1.189575  16.19342  53.221977   \n",
       "\n",
       "   fab_breached  aspect_sin  aspect_cos  d8_accum_log1p  \n",
       "0   1034.973267   -0.001413   -0.999999        3.993086  \n",
       "1   1034.973267   -0.001413   -0.999999        3.993086  \n",
       "2   1034.973267   -0.001413   -0.999999        3.993086  \n",
       "3   1034.973267   -0.001413   -0.999999        3.993086  \n",
       "4   1034.973267   -0.001413   -0.999999        3.993086  \n",
       "5   1034.973267   -0.001413   -0.999999        3.993086  \n",
       "6   1034.973267   -0.001413   -0.999999        3.993086  \n",
       "7   1034.973267   -0.001413   -0.999999        3.993086  \n",
       "8   1034.973267   -0.001413   -0.999999        3.993086  \n",
       "9   1034.973267   -0.001413   -0.999999        3.993086  \n",
       "\n",
       "[10 rows x 157 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rgt</th>\n",
       "      <th>track</th>\n",
       "      <th>spot</th>\n",
       "      <th>height</th>\n",
       "      <th>geoid_height</th>\n",
       "      <th>orthometric_height</th>\n",
       "      <th>h_alos_dem</th>\n",
       "      <th>delta_alos_dem</th>\n",
       "      <th>abs_delta_alos_dem</th>\n",
       "      <th>h_aster_dem</th>\n",
       "      <th>...</th>\n",
       "      <th>cls_nmad_fab</th>\n",
       "      <th>curvature</th>\n",
       "      <th>roughness</th>\n",
       "      <th>tpi</th>\n",
       "      <th>tri</th>\n",
       "      <th>d8_accum</th>\n",
       "      <th>fab_breached</th>\n",
       "      <th>aspect_sin</th>\n",
       "      <th>aspect_cos</th>\n",
       "      <th>d8_accum_log1p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>556.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1069.885864</td>\n",
       "      <td>36.636</td>\n",
       "      <td>1033.249864</td>\n",
       "      <td>1054.0</td>\n",
       "      <td>20.750136</td>\n",
       "      <td>20.750136</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.490112</td>\n",
       "      <td>14.673096</td>\n",
       "      <td>1.189575</td>\n",
       "      <td>16.19342</td>\n",
       "      <td>53.221977</td>\n",
       "      <td>1034.973267</td>\n",
       "      <td>-0.001413</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>3.993086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>556.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1069.885864</td>\n",
       "      <td>36.636</td>\n",
       "      <td>1033.249864</td>\n",
       "      <td>1054.0</td>\n",
       "      <td>20.750136</td>\n",
       "      <td>20.750136</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.490112</td>\n",
       "      <td>14.673096</td>\n",
       "      <td>1.189575</td>\n",
       "      <td>16.19342</td>\n",
       "      <td>53.221977</td>\n",
       "      <td>1034.973267</td>\n",
       "      <td>-0.001413</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>3.993086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>556.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1069.440063</td>\n",
       "      <td>36.636</td>\n",
       "      <td>1032.804063</td>\n",
       "      <td>1044.0</td>\n",
       "      <td>11.195937</td>\n",
       "      <td>11.195937</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.490112</td>\n",
       "      <td>14.673096</td>\n",
       "      <td>1.189575</td>\n",
       "      <td>16.19342</td>\n",
       "      <td>53.221977</td>\n",
       "      <td>1034.973267</td>\n",
       "      <td>-0.001413</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>3.993086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>556.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1069.440063</td>\n",
       "      <td>36.636</td>\n",
       "      <td>1032.804063</td>\n",
       "      <td>1044.0</td>\n",
       "      <td>11.195937</td>\n",
       "      <td>11.195937</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.490112</td>\n",
       "      <td>14.673096</td>\n",
       "      <td>1.189575</td>\n",
       "      <td>16.19342</td>\n",
       "      <td>53.221977</td>\n",
       "      <td>1034.973267</td>\n",
       "      <td>-0.001413</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>3.993086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>556.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1070.128174</td>\n",
       "      <td>36.636</td>\n",
       "      <td>1033.492174</td>\n",
       "      <td>1044.0</td>\n",
       "      <td>10.507826</td>\n",
       "      <td>10.507826</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.490112</td>\n",
       "      <td>14.673096</td>\n",
       "      <td>1.189575</td>\n",
       "      <td>16.19342</td>\n",
       "      <td>53.221977</td>\n",
       "      <td>1034.973267</td>\n",
       "      <td>-0.001413</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>3.993086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>556.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1070.128174</td>\n",
       "      <td>36.636</td>\n",
       "      <td>1033.492174</td>\n",
       "      <td>1044.0</td>\n",
       "      <td>10.507826</td>\n",
       "      <td>10.507826</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.490112</td>\n",
       "      <td>14.673096</td>\n",
       "      <td>1.189575</td>\n",
       "      <td>16.19342</td>\n",
       "      <td>53.221977</td>\n",
       "      <td>1034.973267</td>\n",
       "      <td>-0.001413</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>3.993086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>556.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1066.541626</td>\n",
       "      <td>36.636</td>\n",
       "      <td>1029.905626</td>\n",
       "      <td>1044.0</td>\n",
       "      <td>14.094374</td>\n",
       "      <td>14.094374</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.490112</td>\n",
       "      <td>14.673096</td>\n",
       "      <td>1.189575</td>\n",
       "      <td>16.19342</td>\n",
       "      <td>53.221977</td>\n",
       "      <td>1034.973267</td>\n",
       "      <td>-0.001413</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>3.993086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>556.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1066.242065</td>\n",
       "      <td>36.636</td>\n",
       "      <td>1029.606065</td>\n",
       "      <td>1044.0</td>\n",
       "      <td>14.393935</td>\n",
       "      <td>14.393935</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.490112</td>\n",
       "      <td>14.673096</td>\n",
       "      <td>1.189575</td>\n",
       "      <td>16.19342</td>\n",
       "      <td>53.221977</td>\n",
       "      <td>1034.973267</td>\n",
       "      <td>-0.001413</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>3.993086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>556.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1066.541626</td>\n",
       "      <td>36.636</td>\n",
       "      <td>1029.905626</td>\n",
       "      <td>1044.0</td>\n",
       "      <td>14.094374</td>\n",
       "      <td>14.094374</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.490112</td>\n",
       "      <td>14.673096</td>\n",
       "      <td>1.189575</td>\n",
       "      <td>16.19342</td>\n",
       "      <td>53.221977</td>\n",
       "      <td>1034.973267</td>\n",
       "      <td>-0.001413</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>3.993086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>556.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1066.242065</td>\n",
       "      <td>36.636</td>\n",
       "      <td>1029.606065</td>\n",
       "      <td>1044.0</td>\n",
       "      <td>14.393935</td>\n",
       "      <td>14.393935</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.490112</td>\n",
       "      <td>14.673096</td>\n",
       "      <td>1.189575</td>\n",
       "      <td>16.19342</td>\n",
       "      <td>53.221977</td>\n",
       "      <td>1034.973267</td>\n",
       "      <td>-0.001413</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>3.993086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 157 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T11:54:32.277431Z",
     "start_time": "2025-10-01T11:54:32.269412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "shema_cat = con.execute(\"DESCRIBE SELECT * FROM NMAD_with_embeddings_cls_features\").fetchdf()\n",
    "print([x for x in shema_cat.column_name])"
   ],
   "id": "17454fda71c12cea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rgt', 'track', 'spot', 'height', 'geoid_height', 'orthometric_height', 'h_alos_dem', 'delta_alos_dem', 'abs_delta_alos_dem', 'h_aster_dem', 'delta_aster_dem', 'abs_delta_aster_dem', 'h_copernicus_dem', 'delta_copernicus_dem', 'abs_delta_copernicus_dem', 'h_fab_dem', 'delta_fab_dem', 'abs_delta_fab_dem', 'h_nasa_dem', 'delta_nasa_dem', 'abs_delta_nasa_dem', 'h_srtm_dem', 'delta_srtm_dem', 'abs_delta_srtm_dem', 'h_tan_dem', 'delta_tan_dem', 'abs_delta_tan_dem', 'lulc_class', 'lulc_name', 'alos_dem_slope', 'aster_dem_slope', 'copernicus_dem_slope', 'fab_dem_slope', 'nasa_dem_slope', 'srtm_dem_slope', 'tan_dem_slope', 'alos_dem_geomorphon', 'alos_dem_landform', 'aster_dem_geomorphon', 'aster_dem_landform', 'copernicus_dem_geomorphon', 'copernicus_dem_landform', 'fab_dem_geomorphon', 'fab_dem_landform', 'nasa_dem_geomorphon', 'nasa_dem_landform', 'srtm_dem_geomorphon', 'srtm_dem_landform', 'tan_dem_geomorphon', 'tan_dem_landform', 'alos_dem_2000', 'aster_dem_2000', 'copernicus_dem_2000', 'fab_dem_2000', 'nasa_dem_2000', 'srtm_dem_2000', 'tan_dem_2000', 'alos_dem_stream', 'aster_dem_stream', 'copernicus_dem_stream', 'fab_dem_stream', 'nasa_dem_stream', 'srtm_dem_stream', 'tan_dem_stream', 'alos_dem_twi', 'aster_dem_twi', 'copernicus_dem_twi', 'fab_dem_twi', 'nasa_dem_twi', 'srtm_dem_twi', 'tan_dem_twi', 'x', 'y', 'x_merc', 'y_merc', 'nmad_alos', 'nmad_aster', 'nmad_cop', 'nmad_fab', 'nmad_nasa', 'nmad_srtm', 'nmad_tan', 'emb_001', 'emb_002', 'emb_003', 'emb_004', 'emb_005', 'emb_006', 'emb_007', 'emb_008', 'emb_009', 'emb_010', 'emb_011', 'emb_012', 'emb_013', 'emb_014', 'emb_015', 'emb_016', 'emb_017', 'emb_018', 'emb_019', 'emb_020', 'emb_021', 'emb_022', 'emb_023', 'emb_024', 'emb_025', 'emb_026', 'emb_027', 'emb_028', 'emb_029', 'emb_030', 'emb_031', 'emb_032', 'emb_033', 'emb_034', 'emb_035', 'emb_036', 'emb_037', 'emb_038', 'emb_039', 'emb_040', 'emb_041', 'emb_042', 'emb_043', 'emb_044', 'emb_045', 'emb_046', 'emb_047', 'emb_048', 'emb_049', 'emb_050', 'emb_051', 'emb_052', 'emb_053', 'emb_054', 'emb_055', 'emb_056', 'emb_057', 'emb_058', 'emb_059', 'emb_060', 'emb_061', 'emb_062', 'emb_063', 'emb_064', 'emb_all', 'cls_nmad_fab', 'curvature', 'roughness', 'tpi', 'tri', 'd8_accum', 'fab_breached', 'aspect_sin', 'aspect_cos', 'd8_accum_log1p']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==== НАЛАШТУВАННЯ ====\n",
    "TARGET_MODE = \"absolute\"   # або \"signed\"\n",
    "\n",
    "TARGET_COLS = {\n",
    "    \"signed\":   \"delta_fab_dem\",      # похибка зі знаком (+/-)\n",
    "    \"absolute\": \"abs_delta_fab_dem\",  # абсолютна похибка (>=0)\n",
    "}\n",
    "\n",
    "# ==== ВИБІР ЦІЛІ ====\n",
    "target_col = TARGET_COLS[TARGET_MODE]\n",
    "y = df[target_col].astype(\"float32\")\n",
    "print(f\"[INFO] TARGET_MODE={TARGET_MODE}  ->  y = {target_col}\")\n"
   ],
   "id": "abdce04b409a610"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T14:37:03.941016Z",
     "start_time": "2025-10-01T14:25:46.241352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ============================================================\n",
    "# FABDEM error regression with spatial split (rgt×track)\n",
    "# CatBoostRegressor + optional PCA(embeddings) + CV metrics\n",
    "# ============================================================\n",
    "\n",
    "import os, json, joblib, duckdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "# ---------- корисна утиліта (RMSE із сумісністю версій sklearn) ----------\n",
    "def rmse(y_true, y_pred):\n",
    "    try:\n",
    "        # sklearn >=1.4\n",
    "        from sklearn.metrics import root_mean_squared_error\n",
    "        return float(root_mean_squared_error(y_true, y_pred))\n",
    "    except Exception:\n",
    "        return float(mean_squared_error(y_true, y_pred, squared=False))\n",
    "\n",
    "\n",
    "# ===================== НАЛАШТУВАННЯ ======================\n",
    "TARGET_MODE = \"signed\"   # \"absolute\" або \"signed\"\n",
    "SAVE_DIR    = \"artifacts_fabdem_reg\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Колонки, які відповідають режимам цілі\n",
    "TARGET_COLS = {\n",
    "    \"signed\":   \"delta_fab_dem\",      # похибка зі знаком (+/-)\n",
    "    \"absolute\": \"abs_delta_fab_dem\",  # абсолютна похибка (>=0)\n",
    "}\n",
    "target_col = TARGET_COLS[TARGET_MODE]\n",
    "\n",
    "# PCA по ембедінгам (вимикай, якщо ембедінгів немає)\n",
    "USE_PCA = True\n",
    "PCA_DIM = 64\n",
    "\n",
    "# CatBoost (можеш підкрутити за потреби)\n",
    "CAT_PARAMS = dict(\n",
    "    iterations=6000,\n",
    "    learning_rate=0.025,\n",
    "    depth=7,\n",
    "    l2_leaf_reg=12,\n",
    "    bootstrap_type=\"Bayesian\",\n",
    "    bagging_temperature=0.6,\n",
    "    random_strength=0.3,\n",
    "    task_type=\"GPU\",\n",
    "    devices=\"0\",\n",
    "    gpu_ram_part=0.75,\n",
    "    loss_function=\"Huber:delta=1.0\",\n",
    "    eval_metric=\"RMSE\",\n",
    "    leaf_estimation_method=\"Newton\",\n",
    "    border_count=128,\n",
    "    max_ctr_complexity=3,\n",
    "    one_hot_max_size=32,\n",
    "    random_seed=42,\n",
    "    metric_period=200,\n",
    "    early_stopping_rounds=300\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Мінімальний розмір тестових груп не задаємо — StratifiedGroupKFold сам збалансує\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "# ==========================================================\n",
    "\n",
    "\n",
    "# ---------- 1) Читаємо дані ----------\n",
    "con = duckdb.connect()\n",
    "\n",
    "df = con.execute(\"\"\"\n",
    "WITH base AS (\n",
    "  SELECT\n",
    "    -- групування координати\n",
    "    rgt, track, spot, x, y,\n",
    "\n",
    "    -- FABDEM + цілі для регресії\n",
    "    h_fab_dem, delta_fab_dem, abs(delta_fab_dem) AS abs_delta_fab_dem,\n",
    "\n",
    "    -- інші DEM (висоти)\n",
    "    h_alos_dem, h_aster_dem, h_copernicus_dem, h_nasa_dem, h_srtm_dem, h_tan_dem,\n",
    "\n",
    "    -- базові похідні від рельєфу\n",
    "    fab_dem_slope, fab_dem_twi, fab_dem_2000, fab_dem_stream,\n",
    "\n",
    "    -- розширені рельєфні фічі\n",
    "    aspect_sin, aspect_cos, curvature, roughness, tpi, tri, d8_accum_log1p,\n",
    "    fab_breached,\n",
    "\n",
    "    -- категоріальні\n",
    "    lulc_class, lulc_name, fab_dem_geomorphon, fab_dem_landform,\n",
    "\n",
    "    -- ембедінги (64-канальні)\n",
    "    emb_001, emb_002, emb_003, emb_004, emb_005, emb_006, emb_007, emb_008,\n",
    "    emb_009, emb_010, emb_011, emb_012, emb_013, emb_014, emb_015, emb_016,\n",
    "    emb_017, emb_018, emb_019, emb_020, emb_021, emb_022, emb_023, emb_024,\n",
    "    emb_025, emb_026, emb_027, emb_028, emb_029, emb_030, emb_031, emb_032,\n",
    "    emb_033, emb_034, emb_035, emb_036, emb_037, emb_038, emb_039, emb_040,\n",
    "    emb_041, emb_042, emb_043, emb_044, emb_045, emb_046, emb_047, emb_048,\n",
    "    emb_049, emb_050, emb_051, emb_052, emb_053, emb_054, emb_055, emb_056,\n",
    "    emb_057, emb_058, emb_059, emb_060, emb_061, emb_062, emb_063, emb_064\n",
    "  FROM read_parquet('data/NMAD_with_embeddings_cls_features.parquet')\n",
    "),\n",
    "calc AS (\n",
    "  SELECT\n",
    "    *,\n",
    "    -- список інших DEM (без FAB)\n",
    "    list_sort(list_value(\n",
    "      h_alos_dem, h_aster_dem, h_copernicus_dem, h_nasa_dem, h_srtm_dem, h_tan_dem\n",
    "    )) AS h_others_sorted,\n",
    "\n",
    "    -- корисні суми для mean/std\n",
    "    (h_alos_dem + h_aster_dem + h_copernicus_dem + h_nasa_dem + h_srtm_dem + h_tan_dem) AS h_sum,\n",
    "    (h_alos_dem*h_alos_dem + h_aster_dem*h_aster_dem + h_copernicus_dem*h_copernicus_dem\n",
    "      + h_nasa_dem*h_nasa_dem + h_srtm_dem*h_srtm_dem + h_tan_dem*h_tan_dem) AS h_sum_sq\n",
    "  FROM base\n",
    "),\n",
    "feat AS (\n",
    "  SELECT\n",
    "    *,\n",
    "    -- середнє / медіана / std / діапазон по інших DEM\n",
    "    h_sum / 6.0                                        AS h_other_mean,\n",
    "    (h_others_sorted[3] + h_others_sorted[4]) / 2.0    AS h_other_median,  -- 1-базна індексація\n",
    "    sqrt( h_sum_sq/6.0 - pow(h_sum/6.0, 2) )           AS h_other_std,\n",
    "    greatest(h_alos_dem, h_aster_dem, h_copernicus_dem, h_nasa_dem, h_srtm_dem, h_tan_dem)\n",
    "      - least(h_alos_dem, h_aster_dem, h_copernicus_dem, h_nasa_dem, h_srtm_dem, h_tan_dem)\n",
    "      AS h_other_range,\n",
    "\n",
    "    -- відхилення FABDEM від консенсусу інших\n",
    "    h_fab_dem - (h_sum / 6.0)                          AS fab_minus_mean,\n",
    "    h_fab_dem - ((h_others_sorted[3] + h_others_sorted[4]) / 2.0) AS fab_minus_median\n",
    "  FROM calc\n",
    ")\n",
    "SELECT\n",
    "  *\n",
    "FROM feat\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "\n",
    "# ---------- 2) Ціль ----------\n",
    "if target_col not in df.columns:\n",
    "    raise ValueError(f\"Column '{target_col}' not found. Available target columns: \"\n",
    "                     f\"{[c for c in df.columns if 'delta_fab_dem' in c]}\")\n",
    "y = df[target_col].astype(\"float32\")\n",
    "print(f\"[INFO] TARGET_MODE={TARGET_MODE}  ->  y = '{target_col}'\")\n",
    "# ---------- 3) Групи для spatial split ----------\n",
    "df[\"group_id\"] = df[\"rgt\"].astype(int).astype(str) + \"_\" + df[\"track\"].astype(int).astype(str)\n",
    "\n",
    "\n",
    "# ===== 4) Фічі: авто-формування списків + інженерія =====\n",
    "\n",
    "# 4.1 DEM-списки\n",
    "dem_h = [c for c in df.columns if c.startswith(\"h_\") and c.endswith(\"_dem\")]\n",
    "dem_h_wo_fab = [c for c in dem_h if c != \"h_fab_dem\"]\n",
    "\n",
    "# 4.2 Якщо є індивідуальні DEM — корисно зробити різниці FABDEM–інший DEM\n",
    "#     (часто дає буст якості).\n",
    "for c in dem_h_wo_fab:\n",
    "    newc = f\"fab_minus_{c.replace('h_','').replace('_dem','')}\"\n",
    "    if newc not in df.columns:\n",
    "        df[newc] = df[\"h_fab_dem\"] - df[c]\n",
    "\n",
    "# 4.3 Базові нелінійності/взаємодії (як і раніше)\n",
    "df[\"slope_sq\"]   = df[\"fab_dem_slope\"]**2\n",
    "df[\"slope_sqrt\"] = np.sqrt(np.clip(df[\"fab_dem_slope\"], 0, None))\n",
    "df[\"twi_sq\"]     = df[\"fab_dem_twi\"]**2\n",
    "df[\"twi_sqrt\"]   = np.sqrt(np.clip(df[\"fab_dem_twi\"], 0, None))\n",
    "\n",
    "# якщо в DuckDB вже є d8_accum_log1p, відновимо масштаб і похідні\n",
    "df[\"accum_exp\"]  = np.expm1(df[\"d8_accum_log1p\"])\n",
    "df[\"accum_sqrt\"] = np.sqrt(df[\"accum_exp\"])\n",
    "\n",
    "df[\"slope_x_twi\"]    = df[\"fab_dem_slope\"] * df[\"fab_dem_twi\"]\n",
    "df[\"slope_x_stream\"] = df[\"fab_dem_slope\"] * df[\"fab_dem_stream\"]\n",
    "df[\"twi_x_stream\"]   = df[\"fab_dem_twi\"]   * df[\"fab_dem_stream\"]\n",
    "df[\"slope_x_accum\"]  = df[\"fab_dem_slope\"] * df[\"accum_exp\"]\n",
    "df[\"twi_x_accum\"]    = df[\"fab_dem_twi\"]   * df[\"accum_exp\"]\n",
    "\n",
    "df[\"curv_abs\"]   = df[\"curvature\"].abs()\n",
    "df[\"rough_x_tri\"]= df[\"roughness\"] * df[\"tri\"]\n",
    "df[\"tpi_x_tri\"]  = df[\"tpi\"] * df[\"tri\"]\n",
    "df[\"planar_idx\"] = (df[\"curvature\"].abs() < 0.005).astype(\"int8\")\n",
    "df[\"steep_idx\"]  = (df[\"fab_dem_slope\"]  > 20).astype(\"int8\")\n",
    "\n",
    "# 4.4 Бінінги (робастні квантилі)\n",
    "q  = df[\"fab_dem_2000\"].quantile([0.1,0.3,0.7,0.9]).values\n",
    "df[\"level_bin\"] = pd.cut(df[\"fab_dem_2000\"],\n",
    "                         bins=[-np.inf, q[0], q[1], q[2], q[3], np.inf],\n",
    "                         labels=[\"lev_very_low\",\"lev_low\",\"lev_mid\",\"lev_high\",\"lev_very_high\"])\n",
    "\n",
    "qt = df[\"fab_dem_twi\"].quantile([0.2,0.4,0.6,0.8]).values\n",
    "df[\"twi_bin\"] = pd.cut(df[\"fab_dem_twi\"],\n",
    "                       bins=[-np.inf, qt[0], qt[1], qt[2], qt[3], np.inf],\n",
    "                       labels=[\"twi_vlow\",\"twi_low\",\"twi_mid\",\"twi_high\",\"twi_vhigh\"])\n",
    "\n",
    "qa = df[\"accum_exp\"].replace([np.inf,-np.inf], np.nan).fillna(0).quantile([0.2,0.4,0.6,0.8]).values\n",
    "df[\"accum_bin\"] = pd.cut(df[\"accum_exp\"],\n",
    "                         bins=[-np.inf, qa[0], qa[1], qa[2], qa[3], np.inf],\n",
    "                         labels=[\"acc_vlow\",\"acc_low\",\"acc_mid\",\"acc_high\",\"acc_vhigh\"])\n",
    "\n",
    "# 4.5 Напрямки аспекту — ТІЛЬКИ числові 0/1 (не категорії!)\n",
    "df[\"aspect_quad_NE\"] = ((df[\"aspect_cos\"]>0) & (df[\"aspect_sin\"]>0)).astype(\"int8\")\n",
    "df[\"aspect_quad_NW\"] = ((df[\"aspect_cos\"]<0) & (df[\"aspect_sin\"]>0)).astype(\"int8\")\n",
    "df[\"aspect_quad_SE\"] = ((df[\"aspect_cos\"]>0) & (df[\"aspect_sin\"]<0)).astype(\"int8\")\n",
    "df[\"aspect_quad_SW\"] = ((df[\"aspect_cos\"]<0) & (df[\"aspect_sin\"]<0)).astype(\"int8\")\n",
    "\n",
    "# 4.6 Списки фіч\n",
    "base_num = [\n",
    "    \"h_fab_dem\",\"fab_dem_slope\",\"fab_dem_twi\",\"fab_dem_2000\",\"fab_dem_stream\",\n",
    "    \"aspect_sin\",\"aspect_cos\",\"curvature\",\"roughness\",\"tpi\",\"tri\",\"d8_accum_log1p\",\n",
    "    \"fab_breached\"\n",
    "]\n",
    "\n",
    "# нові з DuckDB про узгодженість (вже пораховані у твоєму SQL)\n",
    "consistency_cols = [c for c in [\n",
    "    \"h_other_mean\",\"h_other_median\",\"h_other_std\",\"h_other_range\",\n",
    "    \"fab_minus_mean\",\"fab_minus_median\"\n",
    "] if c in df.columns]\n",
    "\n",
    "# різниці FABDEM–кожен інший DEM (з п.4.2)\n",
    "per_dem_diff_cols = [c for c in df.columns if c.startswith(\"fab_minus_\") and c not in consistency_cols]\n",
    "\n",
    "added_num = [\n",
    "    \"slope_sq\",\"slope_sqrt\",\"twi_sq\",\"twi_sqrt\",\"accum_exp\",\"accum_sqrt\",\n",
    "    \"slope_x_twi\",\"slope_x_stream\",\"twi_x_stream\",\"slope_x_accum\",\"twi_x_accum\",\n",
    "    \"curv_abs\",\"rough_x_tri\",\"tpi_x_tri\",\"planar_idx\",\"steep_idx\",\n",
    "    \"aspect_quad_NE\",\"aspect_quad_NW\",\"aspect_quad_SE\",\"aspect_quad_SW\"\n",
    "] + consistency_cols + per_dem_diff_cols\n",
    "\n",
    "num_cols = base_num + added_num\n",
    "\n",
    "# категоріальні (лише справді категоріальні!)\n",
    "cat_cols = [\"lulc_class\",\"lulc_name\",\"fab_dem_geomorphon\",\"fab_dem_landform\",\n",
    "            \"level_bin\",\"twi_bin\",\"accum_bin\"]\n",
    "cat_cols = [c for c in cat_cols if c in df.columns]\n",
    "\n",
    "# ембедінги\n",
    "emb_cols = [f\"emb_{i:03d}\" for i in range(1,65) if f\"emb_{i:03d}\" in df.columns]\n",
    "\n",
    "# 4.7 Нормалізація рідкісних категорій\n",
    "def collapse_rare(s: pd.Series, min_frac=0.01) -> pd.Series:\n",
    "    vc = s.value_counts(normalize=True, dropna=False)\n",
    "    rare = vc[vc < min_frac].index\n",
    "    s = s.astype(str)\n",
    "    return s.where(~s.isin(rare), \"__OTHER__\")\n",
    "\n",
    "for c in cat_cols:\n",
    "    df[c] = collapse_rare(df[c], 0.01)\n",
    "\n",
    "# 4.8 Downcast усіх числових фіч до float32 (економія VRAM)\n",
    "num_like = list(dict.fromkeys(num_cols + emb_cols))  # порядок + унікальність\n",
    "for c in num_like:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float32\")\n",
    "\n",
    "\n",
    "\n",
    "# ---------- 5) StratifiedGroupKFold з надійним бінінгом y ----------\n",
    "\n",
    "# 5.1 чистимо y: прибираємо +/-inf -> NaN, і далі фільтруємо\n",
    "y_clean = y.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# групи теж не повинні мати NaN\n",
    "grp_clean = df[\"group_id\"].astype(str)\n",
    "\n",
    "# єдине джерело істини: де y валідний і група валідна\n",
    "mask = y_clean.notna() & grp_clean.notna()\n",
    "if mask.sum() == 0:\n",
    "    raise ValueError(\"Після фільтрації не лишилось зразків без NaN у y/group_id.\")\n",
    "\n",
    "# фільтруємо всі об’єкти, що йдуть у CV\n",
    "df_cv   = df.loc[mask].reset_index(drop=True)\n",
    "y_cv    = y_clean.loc[mask].reset_index(drop=True)\n",
    "groups  = grp_clean.loc[mask].reset_index(drop=True)\n",
    "\n",
    "# 5.2 стратифікаційні біни через qcut (квантильний бінінг)\n",
    "#    n_bins=10 дає нормальну градацію; якщо мало даних -> зменши\n",
    "try:\n",
    "    y_bins = pd.qcut(y_cv.abs(), q=10, labels=False, duplicates=\"drop\")\n",
    "except ValueError:\n",
    "    # якщо дуже мало унікальних значень -> fallback на 5 бінів\n",
    "    y_bins = pd.qcut(y_cv.abs(), q=5, labels=False, duplicates=\"drop\")\n",
    "\n",
    "y_bins = y_bins.astype(int)\n",
    "\n",
    "# 5.3 формуємо фолди на відфільтрованих даних\n",
    "sgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "folds = list(sgkf.split(df_cv.index, y_bins, groups=groups))\n",
    "\n",
    "\n",
    "# ---------- 6) PCA на ембедінги (fit per fold) ----------\n",
    "pca_global = PCA(n_components=PCA_DIM, random_state=RANDOM_STATE) if (USE_PCA and len(emb_cols)>0) else None\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    yt = np.asarray(y_true, dtype=float)\n",
    "    yp = np.asarray(y_pred, dtype=float)\n",
    "    mask = np.isfinite(yt) & np.isfinite(yp)\n",
    "    if mask.sum() == 0:\n",
    "        return float(\"nan\")\n",
    "    return float(np.sqrt(np.mean((yp[mask] - yt[mask])**2)))\n",
    "# ---------- 7) Крос-валідація ----------\n",
    "all_metrics = []\n",
    "test_preds_all = []\n",
    "\n",
    "for k, (tr_idx, te_idx) in enumerate(folds, 1):\n",
    "    print(f\"\\n===== FOLD {k} =====\")\n",
    "\n",
    "    # беремо з відфільтрованих\n",
    "    Xn_tr = df_cv.loc[tr_idx, num_cols].copy()\n",
    "    Xn_te = df_cv.loc[te_idx, num_cols].copy()\n",
    "    Xc_tr = df_cv.loc[tr_idx, cat_cols].astype(str).copy()\n",
    "    Xc_te = df_cv.loc[te_idx, cat_cols].astype(str).copy()\n",
    "    if pca_global is not None:\n",
    "        # копія, щоб мати новий PCA на кожен фолд\n",
    "        pca = PCA(n_components=PCA_DIM, random_state=RANDOM_STATE + k)\n",
    "\n",
    "        Z_tr_raw = df_cv.loc[tr_idx, emb_cols].to_numpy(dtype=\"float32\")\n",
    "        Z_te_raw = df_cv.loc[te_idx, emb_cols].to_numpy(dtype=\"float32\")\n",
    "\n",
    "        Z_tr_raw = np.nan_to_num(Z_tr_raw, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        Z_te_raw = np.nan_to_num(Z_te_raw, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        pca.fit(Z_tr_raw)\n",
    "        if k == 1:\n",
    "            print(f\"PCA EVR cumulative (n_components={pca.n_components_}): {pca.explained_variance_ratio_.sum():.5f}\")\n",
    "\n",
    "        Z_tr = pca.transform(Z_tr_raw)\n",
    "        Z_te = pca.transform(Z_te_raw)\n",
    "\n",
    "        zcols = [f\"pca_emb_{i:02d}\" for i in range(Z_tr.shape[1])]\n",
    "        Xn_tr = pd.concat([Xn_tr, pd.DataFrame(Z_tr, index=Xn_tr.index, columns=zcols)], axis=1)\n",
    "        Xn_te = pd.concat([Xn_te, pd.DataFrame(Z_te, index=Xn_te.index, columns=zcols)], axis=1)\n",
    "\n",
    "    X_tr = pd.concat([Xn_tr, Xc_tr], axis=1)\n",
    "    X_te = pd.concat([Xn_te, Xc_te], axis=1)\n",
    "\n",
    "    cat_idx = [X_tr.columns.get_loc(c) for c in Xc_tr.columns if c in X_tr.columns]\n",
    "\n",
    "    train_pool = Pool(X_tr, y_cv.iloc[tr_idx], cat_features=cat_idx)\n",
    "    test_pool  = Pool(X_te, y_cv.iloc[te_idx], cat_features=cat_idx)\n",
    "\n",
    "    model = CatBoostRegressor(**{k: v for k, v in CAT_PARAMS.items() if k != \"early_stopping_rounds\"})\n",
    "    model.fit(\n",
    "    train_pool,\n",
    "    eval_set=test_pool,\n",
    "    use_best_model=True,\n",
    "    verbose=False,\n",
    "    early_stopping_rounds=CAT_PARAMS.get(\"early_stopping_rounds\", 300))\n",
    "    best_it = model.get_best_iteration()\n",
    "    print(f\"Best iteration (fold {k}):\", best_it)\n",
    "\n",
    "    y_hat = model.predict(test_pool)\n",
    "    y_true_fold = y_cv.iloc[te_idx].to_numpy()\n",
    "    mask = np.isfinite(y_true_fold) & np.isfinite(y_hat)\n",
    "    fold_rmse = rmse(y_true_fold, y_hat)\n",
    "    fold_mae  = float(np.mean(np.abs(y_hat[mask] - y_true_fold[mask])))\n",
    "    fold_r2   = float(r2_score(y_true_fold[mask], y_hat[mask]))\n",
    "\n",
    "    print(\"y_cv NaN rate:\", float(np.mean(~np.isfinite(y_cv))))\n",
    "    print(\"fold0 sizes:\", len(folds[0][0]), len(folds[0][1]))\n",
    "    print(f\"FOLD {k}: RMSE={fold_rmse:.3f}  MAE={fold_mae:.3f}  R²={fold_r2:.3f}\")\n",
    "\n",
    "    all_metrics.append({\"fold\": k, \"rmse\": fold_rmse, \"mae\": fold_mae, \"r2\": fold_r2})\n",
    "\n",
    "    # збережемо прогнози з координатами для карт\n",
    "    out = df_cv.loc[te_idx, [\"x\",\"y\",\"rgt\",\"track\",\"spot\"]].copy()\n",
    "    out[\"y_true\"] = y_cv.iloc[te_idx].values\n",
    "    out[\"y_pred\"] = y_hat\n",
    "    out[\"abs_error\"] = (out[\"y_pred\"] - out[\"y_true\"]).abs()\n",
    "    out[\"fold\"] = k\n",
    "    test_preds_all.append(out)\n",
    "\n",
    "# ---------- 8) Підсумки + артефакти ----------\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "print(\"\\n===== CV SUMMARY =====\")\n",
    "print(\"RMSE:\", metrics_df[\"rmse\"].mean().round(3), \"±\", metrics_df[\"rmse\"].std().round(3))\n",
    "print(\"MAE :\", metrics_df[\"mae\"].mean().round(3),  \"±\", metrics_df[\"mae\"].std().round(3))\n",
    "print(\"R²  :\", metrics_df[\"r2\"].mean().round(3),   \"±\", metrics_df[\"r2\"].std().round(3))\n",
    "\n",
    "# крос-валідаційні прогнози (для карт)\n",
    "test_preds_df = pd.concat(test_preds_all, axis=0).reset_index(drop=True)\n",
    "test_preds_df.to_parquet(os.path.join(SAVE_DIR, \"cv_reg_test_predictions.parquet\"), index=False)\n",
    "metrics_df.to_csv(os.path.join(SAVE_DIR, \"cv_reg_metrics.csv\"), index=False)\n",
    "\n",
    "# збережемо останню модель і (за потреби) PCA\n",
    "model.save_model(os.path.join(SAVE_DIR, f\"catboost_fabdem_reg_last_{TARGET_MODE}.cbm\"))\n",
    "manifest = {\n",
    "    \"target_mode\": TARGET_MODE,\n",
    "    \"target_column\": target_col,\n",
    "    \"numeric_features\": num_cols + ([f\"pca_emb_{i:02d}\" for i in range(PCA_DIM)] if (pca is not None) else []),\n",
    "    \"categorical_features\": cat_cols,\n",
    "    \"embeddings_used\": bool(emb_cols),\n",
    "    \"pca_dim\": int(pca.n_components_) if (pca is not None) else 0,\n",
    "    \"group_field\": \"group_id\",\n",
    "    \"splits\": N_SPLITS,\n",
    "    \"params\": CAT_PARAMS\n",
    "}\n",
    "json.dump(manifest, open(os.path.join(SAVE_DIR, \"columns_manifest.json\"), \"w\"),\n",
    "          ensure_ascii=False, indent=2)\n",
    "\n",
    "if pca is not None:\n",
    "    joblib.dump(pca, os.path.join(SAVE_DIR, f\"pca_embeddings_{PCA_DIM}.pkl\"))\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" - CV metrics  ->\", os.path.join(SAVE_DIR, \"cv_reg_metrics.csv\"))\n",
    "print(\" - Test preds  ->\", os.path.join(SAVE_DIR, \"cv_reg_test_predictions.parquet\"))\n",
    "print(\" - Model (last)->\", os.path.join(SAVE_DIR, f\"catboost_fabdem_reg_last_{TARGET_MODE}.cbm\"))\n",
    "if pca is not None:\n",
    "    print(\" - PCA         ->\", os.path.join(SAVE_DIR, f\"pca_embeddings_{PCA_DIM}.pkl\"))\n",
    "print(\" - Manifest    ->\", os.path.join(SAVE_DIR, \"columns_manifest.json\"))\n"
   ],
   "id": "9033e021f3fd8c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] TARGET_MODE=signed  ->  y = 'delta_fab_dem'\n",
      "\n",
      "===== FOLD 1 =====\n",
      "PCA EVR cumulative (n_components=64): 1.00000\n",
      "Best iteration (fold 1): 5999\n",
      "y_cv NaN rate: 0.0\n",
      "fold0 sizes: 1416074 605284\n",
      "FOLD 1: RMSE=3.991  MAE=2.637  R²=0.607\n",
      "\n",
      "===== FOLD 2 =====\n",
      "Best iteration (fold 2): 2779\n",
      "y_cv NaN rate: 0.0\n",
      "fold0 sizes: 1416074 605284\n",
      "FOLD 2: RMSE=4.578  MAE=3.115  R²=0.651\n",
      "\n",
      "===== FOLD 3 =====\n",
      "Best iteration (fold 3): 5994\n",
      "y_cv NaN rate: 0.0\n",
      "fold0 sizes: 1416074 605284\n",
      "FOLD 3: RMSE=3.412  MAE=2.341  R²=0.660\n",
      "\n",
      "===== FOLD 4 =====\n",
      "Best iteration (fold 4): 5997\n",
      "y_cv NaN rate: 0.0\n",
      "fold0 sizes: 1416074 605284\n",
      "FOLD 4: RMSE=3.805  MAE=2.599  R²=0.621\n",
      "\n",
      "===== FOLD 5 =====\n",
      "Best iteration (fold 5): 5999\n",
      "y_cv NaN rate: 0.0\n",
      "fold0 sizes: 1416074 605284\n",
      "FOLD 5: RMSE=3.615  MAE=2.287  R²=0.564\n",
      "\n",
      "===== CV SUMMARY =====\n",
      "RMSE: 3.88 ± 0.445\n",
      "MAE : 2.596 ± 0.329\n",
      "R²  : 0.62 ± 0.038\n",
      "\n",
      "Saved:\n",
      " - CV metrics  -> artifacts_fabdem_reg\\cv_reg_metrics.csv\n",
      " - Test preds  -> artifacts_fabdem_reg\\cv_reg_test_predictions.parquet\n",
      " - Model (last)-> artifacts_fabdem_reg\\catboost_fabdem_reg_last_signed.cbm\n",
      " - PCA         -> artifacts_fabdem_reg\\pca_embeddings_64.pkl\n",
      " - Manifest    -> artifacts_fabdem_reg\\columns_manifest.json\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c30911c68ffed62e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T21:21:23.703606Z",
     "start_time": "2025-10-01T21:07:36.803702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ============================================================\n",
    "# FABDEM error regression (spatial split rgt×track)\n",
    "# DuckDB feature engineering (consensus across DEMs) + CatBoost\n",
    "# ============================================================\n",
    "\n",
    "import os, json, joblib, duckdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "# ---------- utils ----------\n",
    "def rmse(y_true, y_pred):\n",
    "    yt = np.asarray(y_true, dtype=float)\n",
    "    yp = np.asarray(y_pred, dtype=float)\n",
    "    m = np.isfinite(yt) & np.isfinite(yp)\n",
    "    if m.sum() == 0:\n",
    "        return float(\"nan\")\n",
    "    return float(np.sqrt(np.mean((yp[m] - yt[m]) ** 2)))\n",
    "\n",
    "# ===================== CONFIG ======================\n",
    "TARGET_MODE = \"signed\"   # \"signed\" | \"absolute\"\n",
    "SAVE_DIR    = \"artifacts_fabdem_reg_2\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "TARGET_COLS = {\n",
    "    \"signed\":   \"delta_fab_dem\",\n",
    "    \"absolute\": \"abs_delta_fab_dem\",\n",
    "}\n",
    "target_col = TARGET_COLS[TARGET_MODE]\n",
    "\n",
    "USE_PCA = True\n",
    "PCA_DIM = 64            # 32 теж норм (EVR~0.98), якщо треба менше VRAM\n",
    "\n",
    "CAT_PARAMS = dict(\n",
    "    iterations=6000,\n",
    "    learning_rate=0.025,\n",
    "    depth=7,\n",
    "    l2_leaf_reg=12,\n",
    "    bootstrap_type=\"Bayesian\",\n",
    "    bagging_temperature=0.6,\n",
    "    random_strength=0.3,\n",
    "    task_type=\"GPU\",\n",
    "    devices=\"0\",\n",
    "    gpu_ram_part=0.75,\n",
    "    loss_function=\"RMSE\",\n",
    "    eval_metric=\"RMSE\",\n",
    "    leaf_estimation_method=\"Newton\",\n",
    "    border_count=128,\n",
    "    max_ctr_complexity=3,\n",
    "    one_hot_max_size=32,\n",
    "    random_seed=42,\n",
    "    metric_period=200,\n",
    "    early_stopping_rounds=300\n",
    ")\n",
    "\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "# ===================================================\n",
    "\n",
    "\n",
    "# ---------- 1) DuckDB: читаємо та інженеримо фічі ----------\n",
    "con = duckdb.connect()\n",
    "df = con.execute(r\"\"\"\n",
    "WITH base AS (\n",
    "  SELECT\n",
    "    -- гео/групи\n",
    "    rgt, track, spot, x, y,\n",
    "\n",
    "    -- цілі для регресії\n",
    "    h_fab_dem, delta_fab_dem, abs(delta_fab_dem) AS abs_delta_fab_dem,\n",
    "\n",
    "    -- ВИСОТИ DEM\n",
    "    h_alos_dem, h_aster_dem, h_copernicus_dem, h_nasa_dem, h_srtm_dem, h_tan_dem,\n",
    "\n",
    "    -- БАЗОВІ похідні FAB\n",
    "    fab_dem_slope, fab_dem_twi, fab_dem_2000, fab_dem_stream,\n",
    "\n",
    "    -- ПОХІДНІ ДЛЯ УСІХ DEM (для консенсусів)\n",
    "    alos_dem_2000, aster_dem_2000, copernicus_dem_2000, nasa_dem_2000, srtm_dem_2000, tan_dem_2000,\n",
    "    alos_dem_stream, aster_dem_stream, copernicus_dem_stream, nasa_dem_stream, srtm_dem_stream, tan_dem_stream,\n",
    "    alos_dem_slope,  aster_dem_slope,  copernicus_dem_slope,  nasa_dem_slope,  srtm_dem_slope,  tan_dem_slope,\n",
    "    alos_dem_twi,    aster_dem_twi,    copernicus_dem_twi,    nasa_dem_twi,    srtm_dem_twi,    tan_dem_twi,\n",
    "\n",
    "    -- рельєфні фічі\n",
    "    aspect_sin, aspect_cos, curvature, roughness, tpi, tri, d8_accum_log1p, fab_breached,\n",
    "\n",
    "    -- категоріальні\n",
    "    lulc_class, lulc_name, fab_dem_geomorphon, fab_dem_landform,\n",
    "\n",
    "    -- ембедінги\n",
    "    emb_001, emb_002, emb_003, emb_004, emb_005, emb_006, emb_007, emb_008,\n",
    "    emb_009, emb_010, emb_011, emb_012, emb_013, emb_014, emb_015, emb_016,\n",
    "    emb_017, emb_018, emb_019, emb_020, emb_021, emb_022, emb_023, emb_024,\n",
    "    emb_025, emb_026, emb_027, emb_028, emb_029, emb_030, emb_031, emb_032,\n",
    "    emb_033, emb_034, emb_035, emb_036, emb_037, emb_038, emb_039, emb_040,\n",
    "    emb_041, emb_042, emb_043, emb_044, emb_045, emb_046, emb_047, emb_048,\n",
    "    emb_049, emb_050, emb_051, emb_052, emb_053, emb_054, emb_055, emb_056,\n",
    "    emb_057, emb_058, emb_059, emb_060, emb_061, emb_062, emb_063, emb_064\n",
    "  FROM read_parquet('data/NMAD_with_embeddings_cls_features.parquet')\n",
    "),\n",
    "\n",
    "-- ===== HEIGHT consensus (по інших DEM, БЕЗ FAB) =====\n",
    "height_calc AS (\n",
    "  SELECT\n",
    "    *,\n",
    "    list_sort(list_value(h_alos_dem, h_aster_dem, h_copernicus_dem, h_nasa_dem, h_srtm_dem, h_tan_dem)) AS h_others_sorted,\n",
    "    (h_alos_dem + h_aster_dem + h_copernicus_dem + h_nasa_dem + h_srtm_dem + h_tan_dem) AS h_sum,\n",
    "    (power(h_alos_dem,2)+power(h_aster_dem,2)+power(h_copernicus_dem,2)\n",
    "     + power(h_nasa_dem,2)+power(h_srtm_dem,2)+power(h_tan_dem,2)) AS h_sum_sq\n",
    "  FROM base\n",
    "),\n",
    "height_feat AS (\n",
    "  SELECT\n",
    "    *,\n",
    "    h_sum/6.0                                                       AS h_other_mean,\n",
    "    (h_others_sorted[3] + h_others_sorted[4]) / 2.0                 AS h_other_median,\n",
    "    sqrt(h_sum_sq/6.0 - power(h_sum/6.0, 2))                        AS h_other_std,\n",
    "    greatest(h_alos_dem,h_aster_dem,h_copernicus_dem,h_nasa_dem,h_srtm_dem,h_tan_dem)\n",
    "      - least(h_alos_dem,h_aster_dem,h_copernicus_dem,h_nasa_dem,h_srtm_dem,h_tan_dem) AS h_other_range,\n",
    "    h_fab_dem - (h_sum/6.0)                                         AS fab_minus_mean,\n",
    "    h_fab_dem - ((h_others_sorted[3] + h_others_sorted[4]) / 2.0)   AS fab_minus_median,\n",
    "    (h_fab_dem - (h_sum/6.0)) / nullif(sqrt(h_sum_sq/6.0 - power(h_sum/6.0,2)), 0.0) AS fab_minus_h_z\n",
    "  FROM height_calc\n",
    "),\n",
    "\n",
    "-- ===== HAND(2000) consensus =====\n",
    "hand_calc AS (\n",
    "  SELECT\n",
    "    *,\n",
    "    list_sort(list_value(alos_dem_2000, aster_dem_2000, copernicus_dem_2000, nasa_dem_2000, srtm_dem_2000, tan_dem_2000)) AS hand_others_sorted,\n",
    "    (alos_dem_2000 + aster_dem_2000 + copernicus_dem_2000 + nasa_dem_2000 + srtm_dem_2000 + tan_dem_2000) AS hand_sum,\n",
    "    (power(alos_dem_2000,2)+power(aster_dem_2000,2)+power(copernicus_dem_2000,2)\n",
    "     + power(nasa_dem_2000,2)+power(srtm_dem_2000,2)+power(tan_dem_2000,2)) AS hand_sum_sq\n",
    "  FROM height_feat\n",
    "),\n",
    "hand_feat AS (\n",
    "  SELECT\n",
    "    *,\n",
    "    hand_sum/6.0                                                    AS hand_other_mean,\n",
    "    (hand_others_sorted[3] + hand_others_sorted[4]) / 2.0           AS hand_other_median,\n",
    "    sqrt(hand_sum_sq/6.0 - power(hand_sum/6.0, 2))                  AS hand_other_std,\n",
    "    greatest(alos_dem_2000, aster_dem_2000, copernicus_dem_2000, nasa_dem_2000, srtm_dem_2000, tan_dem_2000)\n",
    "      - least(alos_dem_2000, aster_dem_2000, copernicus_dem_2000, nasa_dem_2000, srtm_dem_2000, tan_dem_2000) AS hand_other_range,\n",
    "    fab_dem_2000 - (hand_sum/6.0)                                   AS fab_minus_hand_mean,\n",
    "    fab_dem_2000 - ((hand_others_sorted[3] + hand_others_sorted[4]) / 2.0) AS fab_minus_hand_median,\n",
    "    (fab_dem_2000 - (hand_sum/6.0)) / nullif(sqrt(hand_sum_sq/6.0 - power(hand_sum/6.0,2)), 0.0) AS fab_minus_hand_z\n",
    "  FROM hand_calc\n",
    "),\n",
    "\n",
    "-- ===== STREAM consensus =====\n",
    "stream_calc AS (\n",
    "  SELECT\n",
    "    *,\n",
    "    list_sort(list_value(alos_dem_stream, aster_dem_stream, copernicus_dem_stream, nasa_dem_stream, srtm_dem_stream, tan_dem_stream)) AS stream_others_sorted,\n",
    "    (alos_dem_stream + aster_dem_stream + copernicus_dem_stream + nasa_dem_stream + srtm_dem_stream + tan_dem_stream) AS stream_sum,\n",
    "    (power(alos_dem_stream,2)+power(aster_dem_stream,2)+power(copernicus_dem_stream,2)\n",
    "     + power(nasa_dem_stream,2)+power(srtm_dem_stream,2)+power(tan_dem_stream,2)) AS stream_sum_sq\n",
    "  FROM hand_feat\n",
    "),\n",
    "stream_feat AS (\n",
    "  SELECT\n",
    "    *,\n",
    "    stream_sum/6.0                                                  AS stream_other_mean,\n",
    "    (stream_others_sorted[3] + stream_others_sorted[4]) / 2.0       AS stream_other_median,\n",
    "    sqrt(stream_sum_sq/6.0 - power(stream_sum/6.0, 2))              AS stream_other_std,\n",
    "    greatest(alos_dem_stream, aster_dem_stream, copernicus_dem_stream, nasa_dem_stream, srtm_dem_stream, tan_dem_stream)\n",
    "      - least(alos_dem_stream, aster_dem_stream, copernicus_dem_stream, nasa_dem_stream, srtm_dem_stream, tan_dem_stream) AS stream_other_range,\n",
    "    fab_dem_stream - (stream_sum/6.0)                               AS fab_minus_stream_mean,\n",
    "    fab_dem_stream - ((stream_others_sorted[3] + stream_others_sorted[4]) / 2.0) AS fab_minus_stream_median,\n",
    "    (fab_dem_stream - (stream_sum/6.0)) / nullif(sqrt(stream_sum_sq/6.0 - power(stream_sum/6.0,2)), 0.0) AS fab_minus_stream_z\n",
    "  FROM stream_calc\n",
    "),\n",
    "\n",
    "-- ===== SLOPE consensus =====\n",
    "slope_calc AS (\n",
    "  SELECT\n",
    "    *,\n",
    "    list_sort(list_value(alos_dem_slope, aster_dem_slope, copernicus_dem_slope, nasa_dem_slope, srtm_dem_slope, tan_dem_slope)) AS slope_others_sorted,\n",
    "    (alos_dem_slope + aster_dem_slope + copernicus_dem_slope + nasa_dem_slope + srtm_dem_slope + tan_dem_slope) AS slope_sum,\n",
    "    (power(alos_dem_slope,2)+power(aster_dem_slope,2)+power(copernicus_dem_slope,2)\n",
    "     + power(nasa_dem_slope,2)+power(srtm_dem_slope,2)+power(tan_dem_slope,2)) AS slope_sum_sq\n",
    "  FROM stream_feat\n",
    "),\n",
    "slope_feat AS (\n",
    "  SELECT\n",
    "    *,\n",
    "    slope_sum/6.0                                                   AS slope_other_mean,\n",
    "    (slope_others_sorted[3] + slope_others_sorted[4]) / 2.0         AS slope_other_median,\n",
    "    sqrt(slope_sum_sq/6.0 - power(slope_sum/6.0, 2))                AS slope_other_std,\n",
    "    greatest(alos_dem_slope, aster_dem_slope, copernicus_dem_slope, nasa_dem_slope, srtm_dem_slope, tan_dem_slope)\n",
    "      - least(alos_dem_slope, aster_dem_slope, copernicus_dem_slope, nasa_dem_slope, srtm_dem_slope, tan_dem_slope) AS slope_other_range,\n",
    "    fab_dem_slope - (slope_sum/6.0)                                  AS fab_minus_slope_mean,\n",
    "    fab_dem_slope - ((slope_others_sorted[3] + slope_others_sorted[4]) / 2.0) AS fab_minus_slope_median,\n",
    "    (fab_dem_slope - (slope_sum/6.0)) / nullif(sqrt(slope_sum_sq/6.0 - power(slope_sum/6.0,2)), 0.0) AS fab_minus_slope_z\n",
    "  FROM slope_calc\n",
    "),\n",
    "\n",
    "-- ===== TWI consensus =====\n",
    "twi_calc AS (\n",
    "  SELECT\n",
    "    *,\n",
    "    list_sort(list_value(alos_dem_twi, aster_dem_twi, copernicus_dem_twi, nasa_dem_twi, srtm_dem_twi, tan_dem_twi)) AS twi_others_sorted,\n",
    "    (alos_dem_twi + aster_dem_twi + copernicus_dem_twi + nasa_dem_twi + srtm_dem_twi + tan_dem_twi) AS twi_sum,\n",
    "    (power(alos_dem_twi,2)+power(aster_dem_twi,2)+power(copernicus_dem_twi,2)\n",
    "     + power(nasa_dem_twi,2)+power(srtm_dem_twi,2)+power(tan_dem_twi,2)) AS twi_sum_sq\n",
    "  FROM slope_feat\n",
    "),\n",
    "twi_feat AS (\n",
    "  SELECT\n",
    "    *,\n",
    "    twi_sum/6.0                                                     AS twi_other_mean,\n",
    "    (twi_others_sorted[3] + twi_others_sorted[4]) / 2.0             AS twi_other_median,\n",
    "    sqrt(twi_sum_sq/6.0 - power(twi_sum/6.0, 2))                    AS twi_other_std,\n",
    "    greatest(alos_dem_twi, aster_dem_twi, copernicus_dem_twi, nasa_dem_twi, srtm_dem_twi, tan_dem_twi)\n",
    "      - least(alos_dem_twi, aster_dem_twi, copernicus_dem_twi, nasa_dem_twi, srtm_dem_twi, tan_dem_twi) AS twi_other_range,\n",
    "    fab_dem_twi - (twi_sum/6.0)                                     AS fab_minus_twi_mean,\n",
    "    fab_dem_twi - ((twi_others_sorted[3] + twi_others_sorted[4]) / 2.0) AS fab_minus_twi_median,\n",
    "    (fab_dem_twi - (twi_sum/6.0)) / nullif(sqrt(twi_sum_sq/6.0 - power(twi_sum/6.0,2)), 0.0) AS fab_minus_twi_z\n",
    "  FROM twi_calc\n",
    ")\n",
    "\n",
    "SELECT * FROM twi_feat;\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "# ---------- 2) target ----------\n",
    "if target_col not in df.columns:\n",
    "    raise ValueError(f\"Column '{target_col}' not found.\")\n",
    "y = df[target_col].astype(\"float32\")\n",
    "print(f\"[INFO] TARGET_MODE={TARGET_MODE} -> y = '{target_col}'\")\n",
    "\n",
    "# ---------- 3) spatial groups ----------\n",
    "df[\"group_id\"] = df[\"rgt\"].astype(int).astype(str) + \"_\" + df[\"track\"].astype(int).astype(str)\n",
    "\n",
    "# ---------- 4) feature lists ----------\n",
    "base_num = [\n",
    "    \"h_fab_dem\",\"fab_dem_slope\",\"fab_dem_twi\",\"fab_dem_2000\",\"fab_dem_stream\",\n",
    "    \"aspect_sin\",\"aspect_cos\",\"curvature\",\"roughness\",\"tpi\",\"tri\",\"d8_accum_log1p\",\n",
    "    \"fab_breached\"\n",
    "]\n",
    "\n",
    "# consensus features arriving from DuckDB\n",
    "consistency_cols = [c for c in [\n",
    "    # height\n",
    "    \"h_other_mean\",\"h_other_median\",\"h_other_std\",\"h_other_range\",\n",
    "    \"fab_minus_mean\",\"fab_minus_median\",\"fab_minus_h_z\",\n",
    "    # hand\n",
    "    \"hand_other_mean\",\"hand_other_median\",\"hand_other_std\",\"hand_other_range\",\n",
    "    \"fab_minus_hand_mean\",\"fab_minus_hand_median\",\"fab_minus_hand_z\",\n",
    "    # stream\n",
    "    \"stream_other_mean\",\"stream_other_median\",\"stream_other_std\",\"stream_other_range\",\n",
    "    \"fab_minus_stream_mean\",\"fab_minus_stream_median\",\"fab_minus_stream_z\",\n",
    "    # slope\n",
    "    \"slope_other_mean\",\"slope_other_median\",\"slope_other_std\",\"slope_other_range\",\n",
    "    \"fab_minus_slope_mean\",\"fab_minus_slope_median\",\"fab_minus_slope_z\",\n",
    "    # twi\n",
    "    \"twi_other_mean\",\"twi_other_median\",\"twi_other_std\",\"twi_other_range\",\n",
    "    \"fab_minus_twi_mean\",\"fab_minus_twi_median\",\"fab_minus_twi_z\",\n",
    "] if c in df.columns]\n",
    "\n",
    "# pairwise FAB - each other DEM (heights)\n",
    "for c in [col for col in df.columns if col.startswith(\"h_\") and col.endswith(\"_dem\") and col != \"h_fab_dem\"]:\n",
    "    name = f\"fab_minus_{c.replace('h_','').replace('_dem','')}\"\n",
    "    if name not in df.columns:\n",
    "        df[name] = df[\"h_fab_dem\"] - df[c]\n",
    "\n",
    "per_dem_diff_cols = [c for c in df.columns if c.startswith(\"fab_minus_\") and c not in consistency_cols]\n",
    "\n",
    "# engineered numerics (in pandas)\n",
    "df[\"slope_sq\"]   = df[\"fab_dem_slope\"]**2\n",
    "df[\"slope_sqrt\"] = np.sqrt(np.clip(df[\"fab_dem_slope\"], 0, None))\n",
    "df[\"twi_sq\"]     = df[\"fab_dem_twi\"]**2\n",
    "df[\"twi_sqrt\"]   = np.sqrt(np.clip(df[\"fab_dem_twi\"], 0, None))\n",
    "df[\"accum_exp\"]  = np.expm1(df[\"d8_accum_log1p\"])\n",
    "df[\"accum_sqrt\"] = np.sqrt(df[\"accum_exp\"])\n",
    "df[\"slope_x_twi\"]    = df[\"fab_dem_slope\"] * df[\"fab_dem_twi\"]\n",
    "df[\"slope_x_stream\"] = df[\"fab_dem_slope\"] * df[\"fab_dem_stream\"]\n",
    "df[\"twi_x_stream\"]   = df[\"fab_dem_twi\"]   * df[\"fab_dem_stream\"]\n",
    "df[\"slope_x_accum\"]  = df[\"fab_dem_slope\"] * df[\"accum_exp\"]\n",
    "df[\"twi_x_accum\"]    = df[\"fab_dem_twi\"]   * df[\"accum_exp\"]\n",
    "df[\"curv_abs\"]   = df[\"curvature\"].abs()\n",
    "df[\"rough_x_tri\"]= df[\"roughness\"] * df[\"tri\"]\n",
    "df[\"tpi_x_tri\"]  = df[\"tpi\"] * df[\"tri\"]\n",
    "df[\"planar_idx\"] = (df[\"curvature\"].abs() < 0.005).astype(\"int8\")\n",
    "df[\"steep_idx\"]  = (df[\"fab_dem_slope\"]  > 20).astype(\"int8\")\n",
    "\n",
    "# robust bins\n",
    "q  = df[\"fab_dem_2000\"].quantile([0.1,0.3,0.7,0.9]).values\n",
    "df[\"level_bin\"] = pd.cut(df[\"fab_dem_2000\"], [-np.inf, q[0], q[1], q[2], q[3], np.inf],\n",
    "                         labels=[\"lev_very_low\",\"lev_low\",\"lev_mid\",\"lev_high\",\"lev_very_high\"])\n",
    "qt = df[\"fab_dem_twi\"].quantile([0.2,0.4,0.6,0.8]).values\n",
    "df[\"twi_bin\"] = pd.cut(df[\"fab_dem_twi\"], [-np.inf, qt[0], qt[1], qt[2], qt[3], np.inf],\n",
    "                       labels=[\"twi_vlow\",\"twi_low\",\"twi_mid\",\"twi_high\",\"twi_vhigh\"])\n",
    "\n",
    "qa = df[\"accum_exp\"].replace([np.inf,-np.inf], np.nan).fillna(0).quantile([0.2,0.4,0.6,0.8]).values\n",
    "df[\"accum_bin\"] = pd.cut(df[\"accum_exp\"], [-np.inf, qa[0], qa[1], qa[2], qa[3], np.inf],\n",
    "                         labels=[\"acc_vlow\",\"acc_low\",\"acc_mid\",\"acc_high\",\"acc_vhigh\"])\n",
    "\n",
    "# aspect quadrants as numeric\n",
    "df[\"aspect_quad_NE\"] = ((df[\"aspect_cos\"]>0) & (df[\"aspect_sin\"]>0)).astype(\"int8\")\n",
    "df[\"aspect_quad_NW\"] = ((df[\"aspect_cos\"]<0) & (df[\"aspect_sin\"]>0)).astype(\"int8\")\n",
    "df[\"aspect_quad_SE\"] = ((df[\"aspect_cos\"]>0) & (df[\"aspect_sin\"]<0)).astype(\"int8\")\n",
    "df[\"aspect_quad_SW\"] = ((df[\"aspect_cos\"]<0) & (df[\"aspect_sin\"]<0)).astype(\"int8\")\n",
    "\n",
    "added_num = [\n",
    "    \"slope_sq\",\"slope_sqrt\",\"twi_sq\",\"twi_sqrt\",\"accum_exp\",\"accum_sqrt\",\n",
    "    \"slope_x_twi\",\"slope_x_stream\",\"twi_x_stream\",\"slope_x_accum\",\"twi_x_accum\",\n",
    "    \"curv_abs\",\"rough_x_tri\",\"tpi_x_tri\",\"planar_idx\",\"steep_idx\",\n",
    "    \"aspect_quad_NE\",\"aspect_quad_NW\",\"aspect_quad_SE\",\"aspect_quad_SW\"\n",
    "]\n",
    "\n",
    "num_cols = base_num + added_num + consistency_cols + per_dem_diff_cols\n",
    "\n",
    "cat_cols = [c for c in [\n",
    "    \"lulc_class\",\"lulc_name\",\"fab_dem_geomorphon\",\"fab_dem_landform\",\n",
    "    \"level_bin\",\"twi_bin\",\"accum_bin\"\n",
    "] if c in df.columns]\n",
    "\n",
    "emb_cols = [f\"emb_{i:03d}\" for i in range(1,65) if f\"emb_{i:03d}\" in df.columns]\n",
    "\n",
    "# rare-category collapse\n",
    "def collapse_rare(s: pd.Series, min_frac=0.01) -> pd.Series:\n",
    "    vc = s.value_counts(normalize=True, dropna=False)\n",
    "    rare = vc[vc < min_frac].index\n",
    "    s = s.astype(str)\n",
    "    return s.where(~s.isin(rare), \"__OTHER__\")\n",
    "\n",
    "for c in cat_cols:\n",
    "    df[c] = collapse_rare(df[c], 0.01)\n",
    "\n",
    "# downcast numerics to float32\n",
    "for c in set(num_cols + emb_cols):\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float32\")\n",
    "\n",
    "# ---------- 5) StratifiedGroupKFold ----------\n",
    "y_clean  = y.replace([np.inf,-np.inf], np.nan)\n",
    "groups   = df[\"group_id\"].astype(str)\n",
    "mask     = y_clean.notna() & groups.notna()\n",
    "\n",
    "df_cv = df.loc[mask].reset_index(drop=True)\n",
    "y_cv  = y_clean.loc[mask].reset_index(drop=True)\n",
    "groups_cv = groups.loc[mask].reset_index(drop=True)\n",
    "\n",
    "try:\n",
    "    y_bins = pd.qcut(y_cv.abs(), q=10, labels=False, duplicates=\"drop\").astype(int)\n",
    "except ValueError:\n",
    "    y_bins = pd.qcut(y_cv.abs(), q=5, labels=False, duplicates=\"drop\").astype(int)\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "folds = list(sgkf.split(df_cv.index, y_bins, groups=groups_cv))\n",
    "\n",
    "# ---------- 6) PCA (per-fold fit) ----------\n",
    "pca_global = PCA(n_components=PCA_DIM, random_state=RANDOM_STATE) if (USE_PCA and len(emb_cols)>0) else None\n",
    "\n",
    "# ---------- 7) CV train ----------\n",
    "all_metrics = []\n",
    "test_preds_all = []\n",
    "\n",
    "for k, (tr_idx, te_idx) in enumerate(folds, 1):\n",
    "    print(f\"\\n===== FOLD {k} =====\")\n",
    "    Xn_tr = df_cv.loc[tr_idx, num_cols].copy()\n",
    "    Xn_te = df_cv.loc[te_idx, num_cols].copy()\n",
    "    Xc_tr = df_cv.loc[tr_idx, cat_cols].astype(str).copy()\n",
    "    Xc_te = df_cv.loc[te_idx, cat_cols].astype(str).copy()\n",
    "\n",
    "    if pca_global is not None:\n",
    "        pca = PCA(n_components=PCA_DIM, random_state=RANDOM_STATE + k)\n",
    "        Z_tr_raw = np.nan_to_num(df_cv.loc[tr_idx, emb_cols].to_numpy(dtype=\"float32\"), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        Z_te_raw = np.nan_to_num(df_cv.loc[te_idx, emb_cols].to_numpy(dtype=\"float32\"), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        pca.fit(Z_tr_raw)\n",
    "        if k == 1:\n",
    "            print(f\"PCA EVR cumulative (n_components={pca.n_components_}): {pca.explained_variance_ratio_.sum():.5f}\")\n",
    "        Z_tr = pca.transform(Z_tr_raw); Z_te = pca.transform(Z_te_raw)\n",
    "        zcols = [f\"pca_emb_{i:02d}\" for i in range(Z_tr.shape[1])]\n",
    "        Xn_tr = pd.concat([Xn_tr, pd.DataFrame(Z_tr, index=Xn_tr.index, columns=zcols)], axis=1)\n",
    "        Xn_te = pd.concat([Xn_te, pd.DataFrame(Z_te, index=Xn_te.index, columns=zcols)], axis=1)\n",
    "\n",
    "    X_tr = pd.concat([Xn_tr, Xc_tr], axis=1)\n",
    "    X_te = pd.concat([Xn_te, Xc_te], axis=1)\n",
    "\n",
    "    cat_idx = [X_tr.columns.get_loc(c) for c in Xc_tr.columns if c in X_tr.columns]\n",
    "\n",
    "    train_pool = Pool(X_tr, y_cv.iloc[tr_idx], cat_features=cat_idx)\n",
    "    test_pool  = Pool(X_te, y_cv.iloc[te_idx],  cat_features=cat_idx)\n",
    "\n",
    "    model = CatBoostRegressor(**{k: v for k, v in CAT_PARAMS.items() if k != \"early_stopping_rounds\"})\n",
    "    model.fit(train_pool, eval_set=test_pool, use_best_model=True, verbose=False,\n",
    "              early_stopping_rounds=CAT_PARAMS.get(\"early_stopping_rounds\", 300))\n",
    "    print(f\"Best iteration (fold {k}):\", model.get_best_iteration())\n",
    "\n",
    "    y_hat = model.predict(test_pool)\n",
    "    y_true = y_cv.iloc[te_idx].to_numpy()\n",
    "    m = np.isfinite(y_true) & np.isfinite(y_hat)\n",
    "    fold_rmse = rmse(y_true, y_hat)\n",
    "    fold_mae  = float(np.mean(np.abs(y_hat[m] - y_true[m])))\n",
    "    fold_r2   = float(r2_score(y_true[m], y_hat[m]))\n",
    "    print(f\"FOLD {k}: RMSE={fold_rmse:.3f}  MAE={fold_mae:.3f}  R²={fold_r2:.3f}\")\n",
    "\n",
    "    all_metrics.append({\"fold\": k, \"rmse\": fold_rmse, \"mae\": fold_mae, \"r2\": fold_r2})\n",
    "\n",
    "    out = df_cv.loc[te_idx, [\"x\",\"y\",\"rgt\",\"track\",\"spot\"]].copy()\n",
    "    out[\"y_true\"] = y_true\n",
    "    out[\"y_pred\"] = y_hat\n",
    "    out[\"abs_error\"] = (out[\"y_pred\"] - out[\"y_true\"]).abs()\n",
    "    out[\"fold\"] = k\n",
    "    test_preds_all.append(out)\n",
    "\n",
    "# ---------- 8) artifacts ----------\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "print(\"\\n===== CV SUMMARY =====\")\n",
    "print(\"RMSE:\", metrics_df[\"rmse\"].mean().round(3), \"±\", metrics_df[\"rmse\"].std().round(3))\n",
    "print(\"MAE :\", metrics_df[\"mae\"].mean().round(3),  \"±\", metrics_df[\"mae\"].std().round(3))\n",
    "print(\"R²  :\", metrics_df[\"r2\"].mean().round(3),   \"±\", metrics_df[\"r2\"].std().round(3))\n",
    "\n",
    "test_preds_df = pd.concat(test_preds_all, axis=0).reset_index(drop=True)\n",
    "test_preds_df.to_parquet(os.path.join(SAVE_DIR, \"cv_reg_test_predictions.parquet\"), index=False)\n",
    "metrics_df.to_csv(os.path.join(SAVE_DIR, \"cv_reg_metrics.csv\"), index=False)\n",
    "\n",
    "model.save_model(os.path.join(SAVE_DIR, f\"catboost_fabdem_reg_last_{TARGET_MODE}.cbm\"))\n",
    "\n",
    "manifest = {\n",
    "    \"target_mode\": TARGET_MODE,\n",
    "    \"target_column\": target_col,\n",
    "    \"numeric_features\": num_cols + ([f\"pca_emb_{i:02d}\" for i in range(PCA_DIM)] if (USE_PCA and len(emb_cols)>0) else []),\n",
    "    \"categorical_features\": cat_cols,\n",
    "    \"embeddings_used\": bool(emb_cols),\n",
    "    \"pca_dim\": int(PCA_DIM if (USE_PCA and len(emb_cols)>0) else 0),\n",
    "    \"group_field\": \"group_id\",\n",
    "    \"splits\": N_SPLITS,\n",
    "    \"params\": CAT_PARAMS\n",
    "}\n",
    "json.dump(manifest, open(os.path.join(SAVE_DIR, \"columns_manifest.json\"), \"w\"), ensure_ascii=False, indent=2)\n",
    "\n",
    "if USE_PCA and len(emb_cols)>0:\n",
    "    joblib.dump(pca, os.path.join(SAVE_DIR, f\"pca_embeddings_{PCA_DIM}.pkl\"))\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" - CV metrics  ->\", os.path.join(SAVE_DIR, \"cv_reg_metrics.csv\"))\n",
    "print(\" - Test preds  ->\", os.path.join(SAVE_DIR, \"cv_reg_test_predictions.parquet\"))\n",
    "print(\" - Model (last)->\", os.path.join(SAVE_DIR, f\"catboost_fabdem_reg_last_{TARGET_MODE}.cbm\"))\n",
    "if USE_PCA and len(emb_cols)>0:\n",
    "    print(\" - PCA         ->\", os.path.join(SAVE_DIR, f\"pca_embeddings_{PCA_DIM}.pkl\"))\n",
    "print(\" - Manifest    ->\", os.path.join(SAVE_DIR, \"columns_manifest.json\"))\n"
   ],
   "id": "ba33af6f7296e146",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] TARGET_MODE=signed -> y = 'delta_fab_dem'\n",
      "\n",
      "===== FOLD 1 =====\n",
      "PCA EVR cumulative (n_components=64): 1.00000\n",
      "Best iteration (fold 1): 5899\n",
      "FOLD 1: RMSE=3.804  MAE=2.582  R²=0.643\n",
      "\n",
      "===== FOLD 2 =====\n",
      "Best iteration (fold 2): 4669\n",
      "FOLD 2: RMSE=4.076  MAE=2.885  R²=0.723\n",
      "\n",
      "===== FOLD 3 =====\n",
      "Best iteration (fold 3): 5977\n",
      "FOLD 3: RMSE=3.337  MAE=2.332  R²=0.675\n",
      "\n",
      "===== FOLD 4 =====\n",
      "Best iteration (fold 4): 5995\n",
      "FOLD 4: RMSE=3.657  MAE=2.559  R²=0.650\n",
      "\n",
      "===== FOLD 5 =====\n",
      "Best iteration (fold 5): 5997\n",
      "FOLD 5: RMSE=3.336  MAE=2.214  R²=0.628\n",
      "\n",
      "===== CV SUMMARY =====\n",
      "RMSE: 3.642 ± 0.317\n",
      "MAE : 2.514 ± 0.259\n",
      "R²  : 0.664 ± 0.037\n",
      "\n",
      "Saved:\n",
      " - CV metrics  -> artifacts_fabdem_reg_2\\cv_reg_metrics.csv\n",
      " - Test preds  -> artifacts_fabdem_reg_2\\cv_reg_test_predictions.parquet\n",
      " - Model (last)-> artifacts_fabdem_reg_2\\catboost_fabdem_reg_last_signed.cbm\n",
      " - PCA         -> artifacts_fabdem_reg_2\\pca_embeddings_64.pkl\n",
      " - Manifest    -> artifacts_fabdem_reg_2\\columns_manifest.json\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "10be7aba22755e91"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
